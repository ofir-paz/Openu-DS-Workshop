{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop In Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project made by:\n",
    "- Ofir Paz\n",
    "- Netanel Ulitszky\n",
    "\n",
    "For the course [\"Workshop in Data Science\"](https://www.openu.ac.il/courses/20936.htm) (20936) at The Open University of Israel.\n",
    "\n",
    "Personal links:\n",
    "<style>\n",
    "table {\n",
    "    border-collapse: collapse;\n",
    "    width: 50%;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "}\n",
    "td {\n",
    "    padding: 6px;\n",
    "    text-align: center;\n",
    "    border-bottom: 1px solid #DDD;\n",
    "}\n",
    "\n",
    "tr:hover {background-color: #333333;}\n",
    "</style>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Ofir Paz</td>\n",
    "        <td>Netanel Ulitszky</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><a href=\"https://www.linkedin.com/in/ofir-paz\">Linkedin</a></td>\n",
    "        <td><a href=\"https://www.linkedin.com/in/netanel-ulitszky\">Linkedin</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><a href=\"https://www.github.com/ofir-paz\">Github</a></td>\n",
    "        <td><a href=\"https://github.com/netane54544\">Github</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><a href=\"https://www.kaggle.com/ofirpaz\">Kaggle</a></td>\n",
    "        <td><a href=\"https://www.kaggle.com/netanelulitszky\">Kaggle</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<!-- style copy paste -->\n",
    "\n",
    "<!--\n",
    "|| (ℹ️) Note\n",
    "||  Content\n",
    " -->\n",
    "<style>\n",
    ".note-box {\n",
    "    border-left: 4px solid #0078D4;\n",
    "    border-image: 1;\n",
    "    padding-left: 10px;\n",
    "    border-radius: 0px;\n",
    "    padding-top: 2px;\n",
    "    padding-bottom: 2px;\n",
    "}\n",
    ".note-header {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    padding-bottom: 0.3em;\n",
    "}\n",
    ".note-icon {\n",
    "    font-size: 14px;\n",
    "    color: #0078D4;\n",
    "    background-color: #1E1E1E;\n",
    "    border: 2px solid #0078D4;\n",
    "    border-radius: 100%;\n",
    "    width: 16px;\n",
    "    height: 16px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    margin-right: 8px;\n",
    "}\n",
    ".note {\n",
    "    color: #0078D4;\n",
    "    font-weight: bold;\n",
    "}\n",
    ".note-content {\n",
    "}\n",
    "</style>\n",
    "\n",
    "<!-- Usage Example\n",
    "\n",
    "<blockquote class=\"note-box\">\n",
    " <div class=\"note-header\">\n",
    "  <span class=\"note-icon\">ℹ️</span><strong class=\"note\">Note</strong> \n",
    " </div>\n",
    "  <span class=\"note-content\">Hello, this is a beautifully styled note with rounded corners.</span>\n",
    "</blockquote>\n",
    "\n",
    " -->\n",
    "<!-- End Note -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "ol {\n",
    "    list-style-position: inside;  /* Move the list marker inside the padding */\n",
    "    padding-left: 2em;  /* Add padding to create the effect of a tab */\n",
    "}\n",
    "</style>\n",
    "<details>\n",
    "    <summary>\n",
    "        1. <a href=\"#introduction\">Introduction</a>\n",
    "    </summary>\n",
    "    <ol>\n",
    "        <li><a href=\"#background\">Background</a></li>\n",
    "        <li><a href=\"#the-competition\">The Competition</a></li>\n",
    "    </ol>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        2. <a href=\"#exploratory-data-analysis\">Exploratory Data Analysis</a>\n",
    "    </summary>\n",
    "    <ol>\n",
    "        <li><a href=\"#imports\">Imports</a></li>\n",
    "        <li><a href=\"#mri-imaging\">MRI Imaging</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#about-mri\">About MRI</a></li>\n",
    "                <li><a href=\"#mri-sequences\">MRI Sequences</a></li>\n",
    "                <li><a href=\"#dicom-files\">Dicom Files</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#data-layout\">Data Layout</a></li>\n",
    "            <ul>\n",
    "                <li><a href=\"#data-description\">Data Description</a></li>\n",
    "                <li><a href=\"#submission\">Submission</a></li>\n",
    "            </ul>\n",
    "        <li><a href=\"#data-analysis\">Data Analysis</a></li>\n",
    "            <ul>\n",
    "                <li><a href=\"#mri-visualization\">MRI Visualization</a></li>\n",
    "                <li><a href=\"#spine-visualization\">Spine Visualization</a></li>\n",
    "                <li><a href=\"#disease-visualization\">Disease Visualization</a></li>\n",
    "                <li><a href=\"#statistical-analysis\">Statistical Analysis</a></li>\n",
    "                <li><a href=\"#image-statistical-analysis\">Image Statistical Analysis</a></li>\n",
    "                <ul>\n",
    "                    <li><a href=\"#diseases-in-the-data\">Diseases in the Data</a></li>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#spinal-canal-stenosis\">Spinal Canal Stenosis</a></li>\n",
    "                        <li><a href=\"#neural-foraminal-narrowing\">Neural Foraminal Narrowing</a></li>\n",
    "                        <li><a href=\"#subarticular-stenosis\">Subarticular Stenosis</a></li>\n",
    "                    </ul>\n",
    "                    <li><a href=\"#mri-image-data\">MRI Image Data</a></li>\n",
    "                </ul>\n",
    "            </ul>\n",
    "    </ol>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        3. <a href=\"#algorithmic-approach\">Algorithmic Approach</a>\n",
    "    </summary>\n",
    "    <ol>\n",
    "        <li><a href=\"#early-discussion\">Early Discussion</a></li>\n",
    "        <li><a href=\"#classic-models\">Classic Models</a></li>\n",
    "        <li><a href=\"#proposed-solution\">Proposed Solution</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#roi-detection-and-classification\">ROI Detection and Classification</a></li>\n",
    "                <li><a href=\"#end-to-end-3d-classification\">End-to-End 3D Classification</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#technical-details\">Technical Details</a></li>\n",
    "            <ul>\n",
    "                <li><a href=\"#data-handling-and-preprocessing\">Data Handling and Preprocessing</a></li>\n",
    "                <li><a href=\"#model-architecture\">Model Architecture</a></li>\n",
    "                <li><a href=\"#training-and-inference\">Training and Inference</a></li>\n",
    "            </ul>\n",
    "        <li><a href=\"#experimentation\">Experimentation</a></li>\n",
    "            <ul>\n",
    "                <li><a href=\"#overview\">Overview</a></li>\n",
    "                <li><a href=\"#best-experiment\">Best Experiment</a></li>\n",
    "                <li><a href=\"#results\">Results</a></li>\n",
    "                <li><a href=\"#model-analysis\">Model Analysis</a></li>\n",
    "            </ul>\n",
    "    </ol>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        4. <a href=\"#summary\">Summary</a>\n",
    "    </summary>\n",
    "    <ol>\n",
    "        <li><a href=\"#our-take\">Our Take</a></li>\n",
    "        <li><a href=\"#workload-distribution\">Workload Distribution</a></li>\n",
    "        <li><a href=\"#side-note\">Side Note</a></li>\n",
    "    </ol>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are Ofir Paz and Netanel Ulitzky, two students from The Open University of Israel, excited to present our project in the \"Workshop in Data Science\" (20936).\n",
    "\n",
    "We are both very interested in data science and are passionate about its endless possibilities, so choosing this course was a natural choice for us. On the other hand, selecting the topic for our project was more challenging for the same reason. We wanted to choose an engaging, challenging, relevant, and valuable topic with significant potential for *exploratory data analysis* (EDA). After a long and thorough search, we have stumbled upon the [\"Kaggle\"](https://www.kaggle.com) platform and found the [\"RSNA 2024 Lumbar Spine Degenerative Classification\"](https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification) competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a medical level, the competition is about helping radiologists diagnose and classify spine degenerative conditions more accurately and efficiently. This can help improve the quality of life for patients with spine degenerative conditions and reduce the time and cost of the diagnosis process.\n",
    "\n",
    "The problem is that the current diagnosis process is very manual and time-consuming and requires a lot of expertise and experience from the radiologist. The radiologist needs to analyze the [MRI](#mri-imaging) images of the patient's lumbar spine and classify the different spine degenerative conditions and their severity. This process is very subjective and can be affected by the radiologist's experience, knowledge, and mood.\n",
    "\n",
    "At a technical specification level, the competition is about classifying lumbar spine [MRI](#mri-imaging) images into five spine degenerative conditions:\n",
    "1. Left Neural Foraminal Narrowing\n",
    "2. Right Neural Foraminal Narrowing\n",
    "3. Left Subarticular Stenosis\n",
    "4. Right Subarticular Stenosis\n",
    "5. Spinal Canal Stenosis\n",
    "\n",
    "Notice that the conditions are divided into two types:\n",
    "- [Neural Foraminal Narrowing](https://my.clevelandclinic.org/health/diseases/24856-foraminal-stenosis)\n",
    "- [Stenosis](https://en.wikipedia.org/wiki/Spinal_stenosis)\n",
    "\n",
    "You can read more about the conditions in the links above.\n",
    "\n",
    "From those conditions, we need to classify **where** the condition is visible between the different spine disc levels (vertebral disc levels): \n",
    "1. L1-L2\n",
    "2. L2-L3\n",
    "3. L3-L4\n",
    "4. L4-L5\n",
    "5. L5-S1\n",
    "\n",
    "With that, we need to provide a severity score for the condition: \n",
    "1. Normal/Mild\n",
    "2. Moderate\n",
    "3. Severe\n",
    "\n",
    "So, we need to provide a classification for the severity of each condition and vertebral disc for each patient; therefore, we need to deliver 25 classifications with three classes each.\n",
    "\n",
    "To end the introduction and start tackling this problem, we are excited to share a note from the competition hosts:\n",
    "\n",
    "<style>\n",
    "@import url(https://fonts.googleapis.com/css?family=Open+Sans:400italic);\n",
    ".blockquote{\n",
    "  width: 75%;\n",
    "  font-family: Open Sans;\n",
    "  font-style: italic;\n",
    "  color: #555555;\n",
    "  border-left: 8px solid #78C0A8 ;\n",
    "  line-height: 1.6;\n",
    "  position: relative;\n",
    "  background: #EDEDED;\n",
    "}\n",
    "</style>\n",
    "<blockquote class=\"blockquote\">\n",
    "On behalf of the Radiological Society of North America (RSNA), welcome to the Lumbar Spine Degenerative Classification Challenge, the latest in a series of challenges RSNA has conducted to promote research in the application of AI to medical imaging.\n",
    "\n",
    "The competition dataset includes more than 2,500 magnetic resonance imaging studies from eight sites on five continents. It has been annotated by a group of more than 50 expert radiologists recruited by the American Society of Neuroradiology.\n",
    "\n",
    "This is a complex challenge, requiring participants to predict the presence and severity of a set of degenerative conditions at each vertebral level of the lumbar spine. In effect, you will try to duplicate the performance of expert radiologists in diagnosing these conditions.\n",
    "\n",
    "There is a great deal of excitement about the use of AI in medical imaging with new tools being approved and implemented at a rapid pace. The launch of this challenge was announced to a receptive audience at the ASNR meeting on May 20, and the winners will be acknowledged at an event during the RSNA annual meeting, the world’s largest gathering of radiologists, in Chicago in early December.\n",
    "\n",
    "We are very grateful for the work you put into advancing the state of the art through your participation in the competition. Good luck!\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a very complicated, medically related dataset, and we need to understand it before we can start working with it. This section will explain the data, the MRI imaging, and the data layout.\n",
    "\n",
    "We will also uncover various statistics about that data through data analysis.\n",
    "\n",
    "But first, we must import the Python libraries we will use in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in modules.\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from math import floor, ceil\n",
    "\n",
    "# Third-party modules.\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Custom modules.\n",
    "import src.config as cfg\n",
    "from src.dataset.spine_dataset import load_dicom_series\n",
    "from src.plots import (\n",
    "    plot_pixel_array,\n",
    "    plot_dicom_series\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRI Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnetic Resonance Imaging ([MRI](https://en.wikipedia.org/wiki/Magnetic_resonance_imaging)) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body.\n",
    "\n",
    "MRI imaging is a powerful tool in the medical field. It can provide detailed images of the body's organs and tissues and help diagnose a wide range of medical conditions, including spine degenerative conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRI Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRI imaging can be used to scan different parts of the body, including the lumbar spine. When generating an MRI image, the radiologist needs to set the sequence of the MRI scanner to generate the desired image. The sequence in an MRI scan refers to the specific set of parameters used to generate the image, which affects the visual appearance of the image and the information it provides.\n",
    "<style>\n",
    ".note-box {\n",
    "    border-left: 4px solid #0078D4;\n",
    "    border-image: 1;\n",
    "    padding-left: 10px;\n",
    "    border-radius: 0px;\n",
    "    padding-top: 2px;\n",
    "    padding-bottom: 2px;\n",
    "}\n",
    ".note-header {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    padding-bottom: 0.3em;\n",
    "}\n",
    ".note-icon {\n",
    "    font-size: 14px;\n",
    "    color: #0078D4;\n",
    "    background-color: #1E1E1E;\n",
    "    border: 2px solid #0078D4;\n",
    "    border-radius: 100%;\n",
    "    width: 16px;\n",
    "    height: 16px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    margin-right: 8px;\n",
    "}\n",
    ".note {\n",
    "    color: #0078D4;\n",
    "    font-weight: bold;\n",
    "}\n",
    ".note-content {\n",
    "}\n",
    "</style>\n",
    "\n",
    "<blockquote class=\"note-box\">\n",
    " <div class=\"note-header\">\n",
    "  <span class=\"note-icon\">ℹ️</span><strong class=\"note\">Note</strong> \n",
    " </div>\n",
    "  <span class=\"note-content\">For further reading about MRI sequences and the specifics of the parameters used to configure one, you can read <a href=\"https://radiopaedia.org/articles/mri-sequence-parameters?lang=us\">this article</a>.</span>\n",
    "</blockquote>\n",
    "\n",
    "Our data contains MRI images with the following sequences:\n",
    "\n",
    "1. **Sagittal T2/STIR**\n",
    "    - Sagittal Plane: This refers to the plane of imaging that divides the body into left and right halves. When imaging is done in the sagittal plane, the images show slices of the body from the side.\n",
    "    - T2-weighted (T2): T2-weighted imaging highlights fluid and edema. In T2 images, fluids appear bright, and soft tissue contrast is well-differentiated. It is particularly useful for detecting abnormalities in tissues with high water content, such as inflammation, cysts, or tumors.\n",
    "    - STIR (Short Tau Inversion Recovery): STIR is a special type of T2-weighted sequence that suppresses the fat signal, making it easier to identify areas of edema or inflammation. It's commonly used in musculoskeletal imaging to assess conditions like bone marrow edema, soft tissue injuries, and other pathologies where fat suppression is important.\n",
    "2. **Sagittal T1**\n",
    "    - Sagittal Plane: As mentioned above, the sagittal plane divides the body into left and right sections.\n",
    "    - T1-weighted (T1): T1-weighted imaging provides good anatomical detail, particularly of fat-containing structures, which appear bright in T1 images. T1-weighted sequences are useful for evaluating anatomical structures, the integrity of tissues, and the presence of fatty lesions. T1 images are often used in conjunction with T2 images for a more comprehensive assessment of tissue contrast.\n",
    "3. **Axial T2**\n",
    "    - Axial Plane: The axial plane slices the body horizontally, dividing it into upper (superior) and lower (inferior) parts. Axial images are viewed as if looking from the feet upwards.\n",
    "    - T2-weighted (T2): As with sagittal T2, axial T2-weighted imaging highlights fluid and is useful for detecting pathology in cross-sectional views. Axial T2 images are often used to evaluate the spinal cord, intervertebral discs, and surrounding soft tissues, as well as to assess brain and abdominal structures.\n",
    "\n",
    "So, to sum things up:\n",
    "- Sagittal T2/STIR: Side view images with emphasis on fluid and inflammation, with fat signal suppression.\n",
    "- Sagittal T1: Side view images focusing on detailed anatomical structures with fat appearing bright.\n",
    "- Axial T2: Horizontal cross-sectional images highlighting fluid and pathology in the body.\n",
    "\n",
    "Before continuing to explore exactly what the data contains, we need to understand the format of the MRI images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicom Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicom (Digital Imaging and Communications in Medicine) is the standard format used to store and transmit medical images, such as MRI images. Dicom files contain both the image data and the metadata associated with the image, such as the patient information, the imaging parameters, and the image acquisition details.\n",
    "\n",
    "MRI images are stored in Dicom files, which have the file extension \".dcm\". Each Dicom file contains a single MRI image, along with the metadata associated with the image. The metadata in a Dicom file can provide valuable information about the image, such as the patient's name, the imaging parameters, and the image acquisition details. We can use this metadata to gain further insights into the MRI images and to understand the characteristics of the lumbar spine degenerative conditions.\n",
    "\n",
    "To load and visualize the Dicom files, we have chosen to use the [pydicom](https://pydicom.github.io/pydicom/stable/index.html#) library, which is a Python library for working with Dicom files. This library provides functions to read and write Dicom files, as well as to extract the metadata from the Dicom files.\n",
    "\n",
    "In the next code block, we will load a sample Dicom file from the dataset and visualize the MRI image and the metadata associated with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dicom_path = cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_SAGITTAL_T1_ID / \"1.dcm\"\n",
    "example_dicom = pydicom.dcmread(example_dicom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the MRI image from the Dicom file, we will use the `pixel_array` attribute of the Dicom file, which contains the pixel data of the image. We will also extract the metadata from the Dicom file and display it to gain insights into the image acquisition details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pixel_array(example_dicom.pixel_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also further enhance the visualization of the MRI image by plotting the entire 3D sequence of images in a 3D plot. This can provide a more comprehensive view of the MRI sequence and help to visualize the spatial relationships between the images. For this, we will use [Plotly](https://plotly.com/python/), a Python library for interactive data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dicom_series(example_dicom_path.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, indeed the sagittal T1 sequence is a side view image focusing on detailed anatomical structures with fat appearing bright (you can see fat tissue at the right side, near the skin of the back). We will later use this visulization tool to understand how the conditions are visible in the MRI images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to establish a correlation between the radiologists' observations and the potential severity of diseases identified in the MRI images. This task necessitates accurate knowledge of the radiologists' predictions concerning the severity of each disease.\n",
    "\n",
    "Each case in our data may include multiple MRI scans: sagittal T2/STIR, Sagittal T1, and Axial T2.\n",
    "Also, each case contains observations about a disease and its specific location. We are provided with three files containing crucial information:\n",
    "\n",
    "1. **Train File:** This file contains the radiologists' predictions for each possible disease severity. A person is diagnosed with one of our diseases when its severity is more than normal mild.\n",
    "2. **Train Series Descriptions File:** Provides x and y coordinates for the observed disease location inside the MRI scan, each type of disease in the study case, and their vertebrae in the lower back. Also, we have the instance id of the MRI image in which the disease has been observed.\n",
    "3. **Train Label Coordinates File:** This file includes the MRI image instance ID and the coordinates of the observed disease within the image, available for every MRI scan in the study case.\n",
    "\n",
    "Each MRI scan is called a 'series' and has a 'series id’. MRI scans are a series of grayscale images that form a video; we call this type of data a 'Three-Dimensional Image'.\n",
    "\n",
    "We want to use machine learning to establish our correlation. To do so, we are provided with a testing file that contains descriptions of each study case series and the series of MRI scan images. Using the testing, we will compare the predictions between the training data and our testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .list-container {\n",
    "        font-family: Arial, sans-serif;\n",
    "        font-size: 14px;\n",
    "        line-height: 1.6;\n",
    "    }\n",
    "    .list-item {\n",
    "        margin-bottom: 25px;\n",
    "        padding-left: 25px;\n",
    "        position: relative;\n",
    "    }\n",
    "    .list-item:before {\n",
    "        content: counter(item) \".\";\n",
    "        counter-increment: item;\n",
    "        position: absolute;\n",
    "        left: 0;\n",
    "        top: 0;\n",
    "        font-size: 20px;\n",
    "        font-weight: bold;\n",
    "        color: #007bff; /* Blue for the numbers */\n",
    "    }\n",
    "    .list-title {\n",
    "        font-weight: bold;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .list-description {\n",
    "        color: #e8e8e8;\n",
    "        margin-bottom: 10px;\n",
    "        display: block;\n",
    "    }\n",
    "    ol {\n",
    "        counter-reset: item;\n",
    "        list-style-type: none;\n",
    "        padding-left: 0;\n",
    "    }\n",
    "    table {\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    th, td {\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "        border-bottom: 1px solid #ddd;\n",
    "    }\n",
    "    th {\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "        color: black;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<ol class=\"list-container\">\n",
    "    <li class=\"list-item\">\n",
    "        <span class=\"list-title\">train.csv</span>\n",
    "        <span class=\"list-description\">This file contains the labels for the training set.</span>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Column</th>\n",
    "                    <th>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><strong>study_id</strong></td>\n",
    "                    <td>A unique identifier for each study. A study may include multiple series of images.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>condition_level</strong></td>\n",
    "                    <td>Target labels representing different conditions and the vertebral level. Examples include <code>spinal_canal_stenosis_l1_l2</code>, where the severity levels are categorized as <code>Normal/Mild</code>, <code>Moderate</code>, or <code>Severe</code>. Some entries may have missing labels.</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li class=\"list-item\">\n",
    "        <span class=\"list-title\">train_label_coordinates.csv</span>\n",
    "        <span class=\"list-description\">This file provides coordinates for specific medical conditions labeled in the images.</span>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Column</th>\n",
    "                    <th>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><strong>study_id</strong></td>\n",
    "                    <td>A unique identifier for each study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>series_id</strong></td>\n",
    "                    <td>A unique identifier for each series of images within a study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>instance_number</strong></td>\n",
    "                    <td>The image's order within the 3D stack, indicating the position of the image in the sequence.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>condition</strong></td>\n",
    "                    <td>The medical condition being identified, including:\n",
    "                        <ul>\n",
    "                            <li><code>spinal_canal_stenosis</code></li>\n",
    "                            <li><code>neural_foraminal_narrowing</code> (left and right sides of the spine)</li>\n",
    "                            <li><code>subarticular_stenosis</code> (left and right sides of the spine)</li>\n",
    "                        </ul>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>level</strong></td>\n",
    "                    <td>The relevant vertebral level, such as <code>l3_l4</code>.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>x</strong></td>\n",
    "                    <td>The x-coordinate for the center of the labeled area.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>y</strong></td>\n",
    "                    <td>The y-coordinate for the center of the labeled area.</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li class=\"list-item\">\n",
    "        <span class=\"list-title\">MRI images of type .dcm</span>\n",
    "        <span class=\"list-description\">We recive train/test_images/study_id/series_id/instance_number.dcm directory structure, the structure is for an MRI image in DICOM format. </span>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Structure</th>\n",
    "                    <th>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><strong>train_images</strong> or <strong>test_images</strong></td>\n",
    "                    <td>Root directories for training or testing images.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>study_id</strong></td>\n",
    "                    <td>Subdirectory for each study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>series_id</strong></td>\n",
    "                    <td>Subdirectory for each series within a study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>instance_number.dcm</strong></td>\n",
    "                    <td>DICOM file representing a single image in the 3D stack.</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li class=\"list-item\">\n",
    "        <span class=\"list-title\">train/test_series_descriptions.csv</span>\n",
    "        <span class=\"list-description\">This file provides descriptions of the MRI scan orientations.</span>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Column</th>\n",
    "                    <th>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><strong>study_id</strong></td>\n",
    "                    <td>A unique identifier for each study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>series_id</strong></td>\n",
    "                    <td>A unique identifier for each series of images within a study.</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td><strong>series_description</strong></td>\n",
    "                    <td>A textual description of the scan's orientation (e.g., axial, sagittal).</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After discussing the datasets, we will proceed to load them and show their contents in real time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load the train data.\n",
    "train_df = pd.read_csv(cfg.TRAIN_CSV_PATH)\n",
    "train_series_descriptions_df = pd.read_csv(cfg.TRAIN_SERIES_DESCRIPTIONS_CSV_PATH)\n",
    "train_label_coordinates_df = pd.read_csv(cfg.TRAIN_LABEL_COORDINATES_CSV_PATH)\n",
    "test_series_descriptions = pd.read_csv(cfg.TEST_SERIES_DESCRIPTIONS_CSV_PATH)\n",
    "\n",
    "# Display the file name and the heads of the DataFrames.\n",
    "print(\"File: train_series_descriptions.csv\")\n",
    "display(train_series_descriptions_df.head(4))\n",
    "\n",
    "print(\"File: train_label_coordinates.csv\")\n",
    "display(train_label_coordinates_df.head(10))\n",
    "\n",
    "print(\"File: train.csv\")\n",
    "display(train_df.head(4).T)\n",
    "\n",
    "print(\"File: test_series_descriptions.csv\")\n",
    "display(test_series_descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data will be used for analysis, so it's important to see how it looks to understand the sections that will appear later. As we can see this is the dataframe that we will work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to submit to the competition an excel file that contains the following:\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    th, td {\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "        border-bottom: 1px solid #ddd;\n",
    "    }\n",
    "    th {\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "        color: black;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Column</th>\n",
    "            <th>Description</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><strong>row_id</strong></td>\n",
    "            <td>A unique identifier combining <code>study_id</code>, <code>condition</code>, and <code>level</code>, e.g., <code>12345_spinal_canal_stenosis_l3_l4</code>.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>normal_mild</strong></td>\n",
    "            <td>Prediction column for the severity level <code>Normal/Mild</code>.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>moderate</strong></td>\n",
    "            <td>Prediction column for the severity level <code>Moderate</code>.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><strong>severe</strong></td>\n",
    "            <td>Prediction column for the severity level <code>Severe</code>.</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We received an example submission file as part of the competition. Our submission must match this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbf = pd.read_csv(cfg.SAMPLE_SUBMISSION_CSV_PATH)\n",
    "\n",
    "display(sbf.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRI Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radiologists' medical studies and experience play a role in identifying spinal diseases. For this spinal MRI, a radiologist would see the following as a potential area of a 'Spinal Canal Stenosis' at the **intervertebral foramen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinal_canal_stenosis_l2_l3_example_path = (\n",
    "    cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_STIR_ID / \"8.dcm\"\n",
    ")\n",
    "spinal_canal_stenosis_l2_l3_example = pydicom.dcmread(spinal_canal_stenosis_l2_l3_example_path).pixel_array\n",
    "spinal_canal_stenosis_l2_l3_example = np.flip(spinal_canal_stenosis_l2_l3_example, axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(spinal_canal_stenosis_l2_l3_example, cmap='gray')\n",
    "plt.title(\"Spinal Canal Stenosis L2/L3 - Not Marked\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(spinal_canal_stenosis_l2_l3_example, cmap='gray')\n",
    "plt.title(\"Spinal Canal Stenosis L2/L3 - Marked\")\n",
    "plt.plot([320.5714285715], [295.7142857143], \"rx\", linewidth=6, markersize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, as mentioned in the introduction, we have three different scans: Sagittal T2/STIR, Sagittal T1 and Axial T2. For now, lets focus in more detail about the meaning of Sagittal and Axial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term sagittal refers to an anatomical plane, which we use to describe the location of structures or the direction of movements.   \n",
    "The Sagittal plane divides the body into right and left sections, perpendicular to the axial plane ( will be explained later ). This is an example of how a Sagittal plane may look:\n",
    "\n",
    "<img src=\"notebook images/sagitalplane.jpg\" width=\"400px\" height=\"400px\">\n",
    "<h6>Healthline. (n.d.). *Sagittal Plane*. Retrieved from https://www.healthline.com/health/body-planes#sagittal-plane\n",
    "</h6>\n",
    "\n",
    "In the picture, it looks like the plane starts from the middle, but in practice, the Sagittal plane in MRI scans are typically taken from left to right to create the slices of each part along the plane. Radiologists using software to observe the stack will see the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dicom_series(cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_SAGITTAL_T2_STIR_ID, cfg.THREEDIM_MRI_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term Axial (or Transverse) also refers to an anatomical plane. The Axial plane divides the body into superior and inferior sections horizontally, perpendicular to the Sagittal plane.  \n",
    "This is an example of how a Sagittal plane may look:\n",
    "\n",
    "<img src=\"notebook images/axialplane.jpg\" width=\"400px\" height=\"400px\">\n",
    "<h6>Healthline. (n.d.). *Transverse Plane*. Retrieved from https://www.healthline.com/health/body-planes#sagittal-plane\n",
    "</h6>\n",
    "\n",
    "While the plane inside the picture is in the middle, the Axial plane in MRI scans is typically taken from top to bottom to create the slices of each part along the plane. Radiologists using software to observe the stack will see the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dicom_series(cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_AXIAL_T2_ID, cfg.THREEDIM_MRI_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to discuss the MRI machine parameters T1 or T2/STIR in detail; those are outside the scope of our project. You can read about them in the link above. \n",
    "\n",
    "However, we're going to show their effect on MRI scans.\n",
    "\n",
    "\n",
    "<img src=\"notebook images/t1t2spine.jpg\" width=\"500px\" height=\"450px\" style=\"display: block; margin-right: auto;\">\n",
    "<h6>Case Western Reserve University School of Medicine. (n.d.). *MRI Basics: T1 vs. T2-weighted Images*. Retrieved from https://case.edu/med/neurology/NR/MRI%20Basics.htm</h6>\n",
    "\n",
    "As we can see, the T1 image brightens the fat in the body.\n",
    "The T2 image brightens fluid and pathology in the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spine Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to learn more about the spine to understand what to look for in each MRI scan.\n",
    "\n",
    "<!--Sharpenning convolution-->\n",
    "<svg style=\"display: none;\">\n",
    "    <defs>\n",
    "        <filter id=\"sharpy\">\n",
    "            <feConvolveMatrix order=\"3\" kernelMatrix=\"0 -1 0 -1 5 -1 0 -1 0\"/>\n",
    "        </filter>\n",
    "    </defs>\n",
    "</svg>\n",
    "\n",
    "This is the spine:\n",
    "<img src=\"notebook images/spineimage.jpg\" width=\"500px\" height=\"400px\" style=\"display: block; margin-right: auto;\">\n",
    "<h6>Herman, A. (2018). Kinematic Analysis of Spinal Cord Injury Animals Treated with a Neurotrophin-Infused Scaffold and Body Weight Supported Treadmill Training. [Figure 2]. 10.13140/RG.2.2.23761.58725.</h6>\n",
    "\n",
    "A disc separates each vertebra. We categorize the different areas in the spine by names, from top to bottom: cervical, Thoracic, Lumbar, and Sacral. We usually shorten the name to the first letter and the vertebra number, for example, S1 (Sacral).\n",
    "\n",
    "In our analysis, we are only interested in the Lumbar and Sacral areas, so we will crop the image to show only the lower spine:\n",
    "<img src=\"notebook images/spineimage_croped.png\" width=\"500px\" height=\"240px\" style=\"filter: url(#sharpy); display: block; margin-right: auto;\">\n",
    "\n",
    "\n",
    "**The intervertebral foramen:**\n",
    "The intervertebral foramen is an oval-shaped opening formed between the pedicles of two adjacent vertebrae of the vertebral column.\n",
    "\n",
    "The pedicles of each vertebra bear small indentations on their superior and inferior aspects, called the superior and inferior vertebral notches, respectively. Between two articulating vertebrae, the superior vertebral notch of the vertebra below and the inferior vertebral notch of the vertebra above form the majority of the boundary of the intervertebral foramen. Parts of the posterior aspects of the vertebral bodies and intervertebral discs, as well as the capsule of the zygapophyseal joints, also contribute to forming the boundaries of the intervertebral foramina. \n",
    "\n",
    "These foramina provide passageways for spinal nerves to carry information to and from the spinal cord.\n",
    "\n",
    "**The intervertebral disc:**\n",
    "The intervertebral disc is essential for the normal functioning of the spine. It is a cushion of fibrocartilage and the principal joint between two vertebrae in the spinal column. There are 23 discs in the human spine: 6 in the cervical region (neck), 12 in the thoracic region (middle back), and 5 in the lumbar region (lower back).  \n",
    "\n",
    "The intervertebral discs allow the spine to be flexible without sacrificing much strength. They also provide a shock-absorbing effect within the spine and prevent the vertebrae from grinding together. They consist of three major components: the inner, nucleus pulposus, the outer, annulus, and the cartilaginous endplates that anchor the discs to adjacent vertebrae.\n",
    "\n",
    "<img src=\"notebook images/spinous.jpg\" width=\"500px\" height=\"400px\" style=\"filter: url(#sharpy); display: block; margin-right: auto;\">\n",
    "<h6>MedlinePlus. (n.d.). Intervertebral disk anatomy. Retrieved from https://medlineplus.gov/ency/imagepages/19469.htm</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disease Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to discuss each disease effect on the spine:\n",
    "\n",
    "1. **Left\\Right Neural Foraminal Narrowing:**\n",
    "Neural foraminal stenosis, or neural foraminal narrowing, is a type of spinal stenosis. \n",
    "It occurs when the small openings between the bones in the spine, called the neural foramina, narrow or tighten.\n",
    "The nerve roots that exit the spinal column through the neural foramina may become compressed, leading to pain, numbness, or weakness.\n",
    "For some people, the condition doesn’t cause symptoms or require treatment. However, severe cases of neural foraminal stenosis can cause paralysis.\n",
    "When both sides of the foraminal canal narrow, it’s referred to as bilateral neural foraminal stenosis.\n",
    "<img src=\"notebook images/Bilateral-Foraminal-Stenosis.png\" width=\"500px\" height=\"400px\" style=\"filter: url(#sharpy); display: block; margin-right: auto;\">\n",
    "<h6>Bonati Spine Institute. (n.d.). *Bilateral Foraminal Stenosis*. Retrieved from https://www.bonati.com/conditions/foraminal-stenosis/bilateral-foraminal-stenosis/\n",
    "</h6>\n",
    "\n",
    "2. **Left\\Right Subarticular Stenosis:**\n",
    "Subarticular Stenosis (aka Lateral Recess Stenosis) is the narrowing of the space within the spinal canal located toward the sides. This passageway for nerves is close to the spinal nerve root, called the lateral recess, or Lee’s entrance.\n",
    "As this type of spinal stenosis worsens, the nerve structures in the lateral recess have less and less clear space around them. Nerves may come in contact with bone, putting pressure on disc material or other tissue, which can cause pain and other symptoms.\n",
    "<img src=\"notebook images/Lateral-Recess-Stenosis.jpg\" width=\"500px\" height=\"400px\" style=\"filter: url(#sharpy); display: block; margin-right: auto;\">\n",
    "<h6>SpineInfo. (n.d.). Lateral Recess Stenosis: Anatomy, Causes, Symptoms, Diagnosis, and Treatment. Retrieved from https://www.spineinfo.com/conditions/lateral-recess-stenosis-anatomy-causes-symptoms-diagnosis-and-treatment/</h6>\n",
    "\n",
    "3. **Spinal Canal Stenosis**\n",
    "Spinal stenosis is narrowing one or more spaces within the spinal canal. \n",
    "The spinal canal is the tunnel that runs through each vertebra in the spine. It contains the spinal cord. Less space within the spinal canal cramps the spinal cord and the nerves that branch off it (nerve roots).\n",
    "A tightened space can cause the spinal cord or nerves to become irritated, compressed, or pinched.\n",
    "<img src=\"notebook images/Canal Stenosis.jpg\" width=\"600px\" height=\"350px\" style=\"filter: url(#sharpy); display: block; margin-right: auto;\">\n",
    "<h6>Spectrum Healthcare. (n.d.). Lumbar Stenosis. Retrieved from https://www.spectrumhealthcare.com.au/blog/?post=lumbar-stenosis</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our model, we must understand the data we are working with. We will do that by gathering statistical information about our data, such as:\n",
    "1. The number of cases in the dataset.\n",
    "2. How the disease looks\n",
    "3. Many data distributions.\n",
    "4. The mean and variance of the anchor points of the conditions visible in the MRI images.\n",
    "5. The mean and variance of the disk levels in which each condition is visible.\n",
    "\n",
    "By acquiring this information, we can better understand the difficulties in the data and devise a way to overcome them before experimenting with training a model.\n",
    "\n",
    "We can go ahead the retrieve these statistics using the [pandas](https://pandas.pydata.org/docs/user_guide/index.html) Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note on this section: during this analysis, all samples were used without dropping any. Usually, when making analysis on large datasets, error and noisy samples are discovered and addressed. In our case, since we work with medical data that is scafolded by experts, the data is already clean and contains no known errors. There are no faulty MRI scans whatsoever, but we use the term \"no known errors\" since there could still be labeling errors, which we can't find without expert knowledge despite our deep dive into the conditions of the spine in the previous sections. The only problem in the data which we could consider noise is that some scans have varying shape of their 2D slices. But this is managed pretty easily with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Number of unique patients.\n",
    "print(f\"Number of unique patients: {train_df['study_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique patients ensures **data validity** by preventing duplicate bias and maintaining statistical integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how the MRI dicom file looks visually.  \n",
    "The following is an example of an MRI scan of the sagittal, axial T2, and sagittal T2/STIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Disease data\n",
    "\n",
    "plot_dicom_series(cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_SAGITTAL_T1_ID, \n",
    "                  cfg.THREEDIM_MRI_SHAPE, plot_title='Example Sagittal T1')\n",
    "plot_dicom_series(cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_AXIAL_T2_ID, \n",
    "                  cfg.THREEDIM_MRI_SHAPE, plot_title='Example Axial T2')\n",
    "plot_dicom_series(cfg.TRAIN_IMAGES_PATH / cfg.EXAMPLE_ID / cfg.EXAMPLE_SAGITTAL_T2_STIR_ID, \n",
    "                  cfg.THREEDIM_MRI_SHAPE, plot_title='Example Sagittal T2/STIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data distributions.\n",
    "\n",
    "color_mapping = {\n",
    "    \"Normal/Mild\": \"#1f77b4\",  # Blue\n",
    "    \"Moderate\": \"#ff7f0e\",     # Orange\n",
    "    \"Severe\": \"#2ca02c\"        # Green\n",
    "}\n",
    "desired_order = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n",
    "\n",
    "figure, axis = plt.subplots(1,3, figsize=(18,5)) \n",
    "for idx, condition_family in enumerate(['foraminal', 'subarticular', 'canal']):\n",
    "    conditions = [condition for condition in train_df.columns if condition_family in condition]\n",
    "    train_condition_family_df = train_df[conditions]\n",
    "    value_counts_df = train_condition_family_df.apply(lambda col: col.value_counts()).T\n",
    "    value_counts_df.rename(mapper=lambda x: x.replace(f'{condition_family}_', ''), axis=0, inplace=True)\n",
    "    value_counts_df = value_counts_df[desired_order]\n",
    "\n",
    "    # Apply the color mapping\n",
    "    colors = [color_mapping[value] for value in value_counts_df.columns]\n",
    "    value_counts_df.plot(kind='bar', stacked=True, ax=axis[idx], color=colors)\n",
    "\n",
    "    axis[idx].set_title(f'{condition_family} distribution', fontsize=18)\n",
    "    axis[idx].tick_params(axis='x', which='major', labelsize=10, rotation=90)\n",
    "    axis[idx].tick_params(axis='y', which='major', labelsize=10, rotation=45)\n",
    "\n",
    "figure.text(0.085, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in question contains almost 2000 cases (1975, to be exact). As a medical dataset, most cases fall under the **Normal/Mild** severity category, with fewer cases classified as **Moderate** severity and even fewer as **Severe** severity. This distribution is particularly noticeable in the **canal stenosis** category, where most cases are of normal/mild severity, with only a few classified as severe. It is crucial to account for this imbalance when training a model, as it may result in the model being biased toward predicting the Normal/Mild class more frequently than the Severe class.\n",
    "\n",
    "Overall, the dataset comprises **25 distinct classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. The mean and variance of the anchor points of the conditions.\n",
    "\n",
    "# Group by condition and level, and calculate mean and variance of x and y coordinates\n",
    "mean_values = train_label_coordinates_df.groupby(['condition', 'level'])[['x', 'y']].mean().reset_index()\n",
    "variance_values = train_label_coordinates_df.groupby(['condition', 'level'])[['x', 'y']].var().reset_index()\n",
    "\n",
    "# Renaming the columns for better readability\n",
    "mean_values.columns = ['condition', 'level', 'mean_x', 'mean_y']\n",
    "variance_values.columns = ['condition', 'level', 'variance_x', 'variance_y']\n",
    "unmarked_correlation = np.corrcoef(np.array(train_label_coordinates_df['x']), np.array(train_label_coordinates_df['y']))[0][1]\n",
    "\n",
    "# Merging the mean and variance data into one dataframe for better presentation\n",
    "result_df = pd.merge(mean_values, variance_values, on=['condition', 'level'])\n",
    "\n",
    "# Creating separate plots for each condition (disease)\n",
    "for condition in result_df['condition'].unique():\n",
    "    condition_data = result_df[result_df['condition'] == condition]\n",
    "    \n",
    "    # Create a new figure with 2 subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot on the first subplot (Mean X with Variance X)\n",
    "    axes[0].errorbar(condition_data['level'], condition_data['mean_x'], \n",
    "                     yerr=condition_data['variance_x']**0.5, fmt='o', color='blue', capsize=5, label='Mean X')\n",
    "    axes[0].set_title(f'Mean and Variance of X Coordinates for {condition}')\n",
    "    axes[0].set_xlabel('Level')\n",
    "    axes[0].set_ylabel('X Coordinate Values')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot on the second subplot (Mean Y with Variance Y)\n",
    "    axes[1].errorbar(condition_data['level'], condition_data['mean_y'], \n",
    "                     yerr=condition_data['variance_y']**0.5, fmt='o', color='green', capsize=5, label='Mean Y')\n",
    "    axes[1].set_title(f'Mean and Variance of Y Coordinates for {condition}')\n",
    "    axes[1].set_xlabel('Level')\n",
    "    axes[1].set_ylabel('Y Coordinate Values')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs illustrate the mean and variance of X and Y coordinates for different spinal conditions across various vertebral levels (L1/L2 to L5/S1). The mean values (dots) indicate the general position where abnormalities are typically identified, while the error bars represent variance, showing how much these positions fluctuate across different MRI scans. \n",
    "  \n",
    "The **mean** and **variance** of anchor points help analyze the spatial distribution of anatomical landmarks for each **condition and level**. The **mean** provides the central tendency, while the **variance** shows consistency or dispersion. This helps:  \n",
    "\n",
    "- Identify **pattern differences** between conditions.  \n",
    "- Assess **landmark stability** with error bars.  \n",
    "- Detect **correlation** between x and y shifts.  \n",
    "- Ensure **annotation consistency** and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. The mean and variance of the disk levels in which each condition is visible.\n",
    "\n",
    "non_normal_df = []\n",
    "\n",
    "# Iterate through severity columns and find non \"Normal/Mild\" entries\n",
    "severity_columns = train_df.columns[1:]  # All severity columns except 'study_id'\n",
    "\n",
    "for col in severity_columns:\n",
    "    # Extract the condition by removing the last two parts (which correspond to the level)\n",
    "    condition_parts = col.split('_')\n",
    "    condition = '_'.join(condition_parts[:-2])  # Join all parts except the last two (which are levels)\n",
    "    condition = condition.replace('_', ' ')\n",
    "    \n",
    "    # Extract the level in the format `l1_l2`, which matches `filtered_df`\n",
    "    level = condition_parts[-2].lower() + '_' + condition_parts[-1].lower()\n",
    "    \n",
    "    for index, row in train_df.iterrows():\n",
    "        if row[col] != 'Normal/Mild':  # If the severity is not 'Normal/Mild'\n",
    "            non_normal_df.append({'study_id': row['study_id'], 'condition': condition, 'level': level, 'severity': row[col]})\n",
    "\n",
    "\n",
    "non_normal_df = pd.DataFrame(non_normal_df)\n",
    "non_normal_df['condition'] = non_normal_df['condition'].str.lower()\n",
    "non_normal_df['level'] = non_normal_df['level'].str.lower()\n",
    "\n",
    "train_label_coordinates_df['condition'] = train_label_coordinates_df['condition'].str.lower()\n",
    "train_label_coordinates_df['level'] = train_label_coordinates_df['level'].str.lower()\n",
    "train_label_coordinates_df['level'] = train_label_coordinates_df['level'].str.replace('/', '_')\n",
    "\n",
    "merged_non_normal_df = pd.merge(\n",
    "    non_normal_df,\n",
    "    train_label_coordinates_df[['study_id', 'condition', 'level', 'instance_number', 'x', 'y']],\n",
    "    on=['study_id', 'condition', 'level'],\n",
    "    how='left'  # This ensures we only bring over the matching rows\n",
    ")\n",
    "\n",
    "\n",
    "merged_non_normal_df = merged_non_normal_df.dropna(subset=['instance_number'])\n",
    "merged_non_normal_df['instance_number'] = merged_non_normal_df['instance_number'].astype(int)\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean_values = merged_non_normal_df.groupby(['condition', 'level'])[['x', 'y']].mean().reset_index()\n",
    "variance_values = merged_non_normal_df.groupby(['condition', 'level'])[['x', 'y']].var().reset_index()\n",
    "marked_correlation = np.corrcoef(np.array(merged_non_normal_df['x']), np.array(merged_non_normal_df['y']))[0][1]\n",
    "\n",
    "# Renaming the columns for better readability\n",
    "mean_values.columns = ['condition', 'level', 'mean_x', 'mean_y']\n",
    "variance_values.columns = ['condition', 'level', 'variance_x', 'variance_y']\n",
    "\n",
    "# Merging the mean and variance data into one dataframe for better presentation\n",
    "result_df = pd.merge(mean_values, variance_values, on=['condition', 'level'])\n",
    "\n",
    "# Creating separate plots for each condition that doesn't have 'Normal/Mild'\n",
    "for condition in result_df['condition'].unique():\n",
    "    condition_data = result_df[result_df['condition'] == condition]\n",
    "    \n",
    "    # Create a new figure for each condition\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the mean of X coordinates as dots without lines\n",
    "    plt.errorbar(condition_data['level'], condition_data['mean_x'], \n",
    "                 yerr=condition_data['variance_x']**0.5, fmt='o', color='blue', capsize=5, label='Mean X')\n",
    "    \n",
    "    # Plot the mean of Y coordinates as dots without lines\n",
    "    plt.errorbar(condition_data['level'], condition_data['mean_y'], \n",
    "                 yerr=condition_data['variance_y']**0.5, fmt='o', color='green', capsize=5, label='Mean Y')\n",
    "    \n",
    "    # Label for X variance (error bars)\n",
    "    plt.errorbar(condition_data['level'], condition_data['mean_x'], \n",
    "                 yerr=condition_data['variance_x']**0.5, fmt='none', color='blue', capsize=5, label='Variance X')\n",
    "    \n",
    "    # Label for Y variance (error bars)\n",
    "    plt.errorbar(condition_data['level'], condition_data['mean_y'], \n",
    "                 yerr=condition_data['variance_y']**0.5, fmt='none', color='green', capsize=5, label='Variance Y')\n",
    "    \n",
    "    # Add labels, title, and legend\n",
    "    plt.title(f'Mean and Variance of X and Y Coordinates for {condition}')\n",
    "    plt.xlabel('Level')\n",
    "    plt.ylabel('Coordinate Values (X and Y)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot for the current condition\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a couple of observations:\n",
    "1. The data variance for the x and y overlap more when each condition is visible.\n",
    "2. The correlation between x and y coordinates is stronger in cases with visible disease than those without. This suggests that the presence of disease introduces a more consistent and predictable relationship between these coordinates, possibly due to anatomical changes associated with the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The correlation when there are non visable diseases in the data: {unmarked_correlation:.3}\")\n",
    "print(f\"The correlation when there are only visable diseases in the data: {marked_correlation:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons could be:\n",
    "- With visible disease: The stronger correlation (0.789) may indicate that in the presence of disease, the anatomical changes in the area vertebrae lead to a more predictable relationship between x and y. This could be due to physical narrowing or other consistent patterns the disease introduces.\n",
    "\n",
    "- Without visible disease: The lower correlation (0.636) might suggest that in the absence of significant disease, the variability in the anatomy is greater, leading to a less predictable relationship between x and y. The structures are perhaps more flexible or variable when no significant disease-related constraint exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the observations:\n",
    "\n",
    "3. We have a high variance, which means the spread of the x and y marked locations is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique conditions from the data\n",
    "conditions = merged_non_normal_df['condition'].unique()\n",
    "\n",
    "# Create a plot for each condition\n",
    "for condition in conditions:\n",
    "    # Filter data by condition\n",
    "    train_condition_df = train_label_coordinates_df[train_label_coordinates_df['condition'] == condition]\n",
    "    \n",
    "    # Create figure with two subplots (one for x coordinates and one for y coordinates)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Plot for X coordinates\n",
    "    ax.scatter(train_condition_df['x'], train_condition_df['y'], color='red', alpha=0.5, label='Observation')\n",
    "    ax.set_title(f'Spread for {condition}')\n",
    "    ax.set_xlabel('X Coordinates')\n",
    "    ax.set_ylabel('Y Coordinates')\n",
    "    ax.legend()\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot visualizes the **spatial distribution** of annotated points in MRI scans, with each dot representing an observation based on **X and Y coordinates**. The **diagonal clustering** suggests a structured pattern, likely following anatomical alignment. Denser regions indicate **frequent occurrences**, while outliers may represent rare cases or annotation inconsistencies.\n",
    "  \n",
    "This spread in the coordinates represents the unique anatomical variations among individuals. Such variations could be due to typical differences in body structure, abnormalities, deformities, or degenerative changes. The diversity in the dataset could pose challenges in accurately identifying conditions such as spinal canal stenosis and neural foraminal narrowing, as the subtle variations in geometry between normal and pathological conditions can overlap.\n",
    "It may also be valuable to consider which features contribute most to this spread and whether certain variables (such as specific spatial coordinates or vertebrae positions) consistently correlate with the pathologies. Identifying key features could reduce sensitivity to noise in the data caused by natural anatomical variations.\n",
    "Also, we must account for noise in the data that can cause higher variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local copy to avoid modifying the original data\n",
    "train_df_local = train_df.copy()\n",
    "\n",
    "# Select columns 1 to 25 as severity columns\n",
    "severity_columns = train_df.columns[1:26]\n",
    "\n",
    "# Apply encoding to the severity columns on the local copy\n",
    "severity_encoding = {\n",
    "    'Normal/Mild': 1,\n",
    "    'Moderate': 2,\n",
    "    'Severe': 3\n",
    "}\n",
    "\n",
    "train_df_local[severity_columns] = train_df_local[severity_columns].replace(severity_encoding)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix_local = train_df_local[severity_columns].corr(method='pearson')\n",
    "\n",
    "# Plotting the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_local, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.tick_params(left=False, bottom=False)\n",
    "plt.title('Pearson Correlation Matrix for Spine Conditions', size=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Pearson correlation matrix visualizes the relationships between different spinal conditions across vertebral levels, with each cell representing the correlation coefficient between two conditions.  \n",
    "\n",
    "Values range from -1 to 1, where:  \n",
    "- 1 indicates a perfect positive correlation  \n",
    "- -1 indicates a perfect negative correlation  \n",
    "- 0 means no correlation  \n",
    "\n",
    "The diagonal values are all 1, as each condition is perfectly correlated with itself.  \n",
    "\n",
    "Stronger correlations, shown in red regions, indicate that certain conditions frequently co-occur, suggesting a possible clinical or anatomical relationship. Weaker correlations, represented by blue regions, suggest greater independence between conditions. The off-diagonal patterns highlight dependencies between different abnormalities, which can inform feature selection, predictive modeling, and clinical diagnostics by identifying common comorbidities.  \n",
    "  \n",
    "This analysis provides valuable insights into the progression and clustering of spine diseases, particularly focusing on conditions such as spinal canal stenosis, neural foraminal narrowing, and subarticular stenosis.\n",
    "\n",
    "1. **Strong Correlations Between Adjacent Vertebral Levels**\n",
    "A key observation from the analysis is that adjacent vertebral levels exhibit stronger correlations in terms of disease severity. For instance, spinal canal stenosis at L2/L3 shows a moderate to strong correlation (0.46) with stenosis at L3/L4. This pattern suggests that when stenosis affects one spinal level, there is a notable likelihood of its presence at the adjacent level. This could indicate a common degenerative process affecting neighboring regions of the spine due to similar biomechanical or anatomical stressors.\n",
    "\n",
    "2. **Weaker Correlations Across Distant Levels**\n",
    "As the analysis moves across non-adjacent spinal levels, the correlation between conditions weakens significantly. For example, the correlation between spinal canal stenosis at L1/L2 and L5/S1 is notably low (0.05). This suggests that the presence or severity of conditions at one end of the spine may have little direct impact on the distant levels. This may reflect the localized nature of many degenerative processes, where disease progression is confined to specific regions unless systemic factors are at play.\n",
    "\n",
    "3. **Moderate Correlations within Neural Foraminal Narrowing**\n",
    "Neural foraminal narrowing exhibits moderate correlations both within and across vertebral levels. For example, left and right neural foraminal narrowing at the same level tend to be moderately correlated, indicating that the condition may often affect both sides symmetrically. Moreover, there are moderate correlations between foraminal narrowing and spinal canal stenosis, particularly at levels like L3/L4. This relationship suggests that as spinal canal stenosis progresses, foraminal narrowing may also worsen, which can further exacerbate nerve compression and clinical symptoms.\n",
    "\n",
    "4. **High Correlations in Subarticular Stenosis Between Left and Right Sides**\n",
    "Subarticular stenosis demonstrates strong correlations between left and right sides at the same spinal levels. This is particularly evident in levels such as L3/L4 and L4/L5, where the condition seems to manifest symmetrically. This symmetry is an important observation as it may guide both diagnostic approaches and surgical interventions, ensuring that clinicians address bilateral stenosis even when symptoms may present more prominently on one side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine our MRI dicom image data.\n",
    "Let us start by looking at the following:\n",
    "1. Relevant dicom meta-data, and meta-data anlysis\n",
    "2. The heatmap of the image width and height for each MRI scan.\n",
    "3. The histogram of the image depth for each MRI scan.\n",
    "4. Exploration for each disease in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the dicom metadata and the number of unique slices for each scan.\n",
    "Dicom files have metadata attached to them. Metadata is important for determining the condition of an MRI image.\n",
    "The metadata of a dicom file can vary; some files may have more metadata, and others may have less.\n",
    "\n",
    "We want to analyze the number of slices and images taken for one MRI scan. This is the depth of the MRI image, which is, as we explained before, a 3D image.\n",
    "Knowing this will help us study the MRI image data ( as will be seen later )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dicom metadata for each scan\n",
    "def process_dicom_file(dicom_file_path: str) -> tuple[str, str]:\n",
    "    try:\n",
    "        # Load DICOM file with metadata only, without pixel data\n",
    "        dicom_data = pydicom.dcmread(dicom_file_path, stop_before_pixels=True)\n",
    "\n",
    "        # Get the series description (or use 'Unknown' if not available)\n",
    "        scan_type = dicom_data.SeriesDescription if hasattr(dicom_data, 'SeriesDescription') else None\n",
    "        \n",
    "        # Skip if the scan type is None or empty\n",
    "        if scan_type and scan_type.strip():\n",
    "            return scan_type, dicom_data  # Return the scan type and the dicom metadata itself\n",
    "        else:\n",
    "            return None, dicom_file_path  # Return None for empty scan type\n",
    "    except Exception as e:\n",
    "        # Return error state, can log error details\n",
    "        return 'Error', dicom_file_path\n",
    "\n",
    "def load_dicom_metadata_by_scan_type(base_dir: str, max_workers: int) -> Dict[str, List[pydicom.dataset.FileDataset]]:\n",
    "    dicom_metadata = {}\n",
    "    dicom_files = []\n",
    "\n",
    "    # Count the total number of DICOM files\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                dicom_files.append(os.path.join(root, file))\n",
    "\n",
    "    total_files = len(dicom_files)\n",
    "    \n",
    "    # Progress bar using tqdm\n",
    "    with tqdm(total=total_files, desc=\"Processing DICOM files\") as pbar:\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_dicom_file, file_path) for file_path in dicom_files]\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    scan_type, dicom_data = future.result()\n",
    "                    if scan_type and scan_type != 'Error':  # Skip if scan_type is None\n",
    "                        if scan_type not in dicom_metadata:\n",
    "                            dicom_metadata[scan_type] = []\n",
    "                        dicom_metadata[scan_type].append(dicom_data)  # Store the dicom_data itself\n",
    "                except Exception as exc:\n",
    "                    print(f\"Exception during processing: {exc}\")\n",
    "                finally:\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    return dicom_metadata\n",
    "\n",
    "# Counting Dicom stacks for each scan\n",
    "def count_dicom_files(folder_path: str) -> Tuple[str, int]:\n",
    "    try:\n",
    "        # Find the first DICOM file in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".dcm\"):\n",
    "                dicom_file_path = os.path.join(folder_path, file)\n",
    "                dicom_data = pydicom.dcmread(dicom_file_path, stop_before_pixels=True)\n",
    "\n",
    "                # Get the series description (scan type) from the DICOM file\n",
    "                scan_type = dicom_data.SeriesDescription if hasattr(dicom_data, 'SeriesDescription') and dicom_data.SeriesDescription.strip() else None\n",
    "                \n",
    "                # Skip if the scan type is empty\n",
    "                if not scan_type:\n",
    "                    return None, 0\n",
    "\n",
    "                # Count the number of DICOM files in the folder\n",
    "                num_files = len([f for f in os.listdir(folder_path) if f.endswith(\".dcm\")])\n",
    "\n",
    "                return scan_type, num_files\n",
    "\n",
    "        # If no DICOM file is found\n",
    "        return None, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {folder_path}: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def scan_dicom_by_scan_type(base_dir: str, max_workers: int) -> defaultdict[str, set]:\n",
    "    dicom_count = defaultdict(set)\n",
    "\n",
    "    # Traverse all folders (PatientID -> SeriesInstanceUID -> DICOM files)\n",
    "    series_folders = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if any(file.endswith(\".dcm\") for file in files):\n",
    "            series_folders.append(root)\n",
    "\n",
    "    total_folders = len(series_folders)\n",
    "\n",
    "    with tqdm(total=total_folders, desc=\"Processing DICOM folders\") as pbar:\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(count_dicom_files, folder) for folder in series_folders]\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    scan_type, num_files = future.result()\n",
    "                    if scan_type and num_files > 0:  # Valid scan type and file count\n",
    "                        dicom_count[scan_type].add(num_files)  # Add file count to the set\n",
    "                except Exception as exc:\n",
    "                    print(f\"Exception during processing: {exc}\")\n",
    "                finally:\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    return dicom_count\n",
    "\n",
    "\n",
    "dicom_metadata = load_dicom_metadata_by_scan_type(cfg.TRAIN_IMAGES_PATH, 6)\n",
    "dicom_count = scan_dicom_by_scan_type(cfg.TRAIN_IMAGES_PATH, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us analyze the dicom metadata.\n",
    "The data we received most of the time looks similar to this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"T1 scan metadata: \")\n",
    "print(dicom_metadata[\"T1\"][0])\n",
    "print()\n",
    "print(\"T2 scan metadata: \")\n",
    "print(dicom_metadata[\"T2\"][0])\n",
    "print()\n",
    "print(\"STIR scan metadata: \")\n",
    "print(dicom_metadata[\"STIR\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is important information here.\n",
    "The data in the metadata we want to focus on:\n",
    "1. Instance Number ( which slice we are using ) \n",
    "2. Rows and Columns ( height and width )\n",
    "3. Pixel Spacing\n",
    "4. Series Description ( scan type ), Patient ID, and UID of the Series Instance\n",
    "\n",
    "Using this data, we can draw more information from the images we can use when needed.\n",
    "Using the Rows and Columns, we can figure out which pixel size our algorithm needs while minimizing the interpolation or padding of images, which is costly and ruins the quality.\n",
    "The pixel spacing will help us adjust our algorithm to handle such images.\n",
    "The Series Description, Patient ID, and UID of the Series Instance can help us organize the data for usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. The heatmap of the image width and height for each MRI scan.\n",
    "\n",
    "def create_heatmaps_for_width_and_height(dicom_metadata) -> None:\n",
    "    scan_type_labels = list(dicom_metadata.keys())\n",
    "    \n",
    "    # Collect width and height data for each scan type\n",
    "    width_height_data = {}\n",
    "    for scan_type, dicom_files in dicom_metadata.items():\n",
    "        widths = []\n",
    "        heights = []\n",
    "        for dicom_file in dicom_files:\n",
    "            # Extract width (Columns) and height (Rows) from each DICOM file\n",
    "            if hasattr(dicom_file, 'Columns') and hasattr(dicom_file, 'Rows'):\n",
    "                widths.append(dicom_file.Columns)\n",
    "                heights.append(dicom_file.Rows)\n",
    "\n",
    "        if widths and heights:\n",
    "            width_height_data[scan_type] = (widths, heights)\n",
    "    \n",
    "    # Create heatmaps for width and height distributions\n",
    "    num_scans = len(scan_type_labels)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_scans, figsize=(5 * num_scans, 6))\n",
    "    fig.suptitle('Width vs. Height Heatmaps by MRI Scan Type')\n",
    "\n",
    "    if num_scans == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable for single scan type\n",
    "\n",
    "    for i, scan_type in enumerate(scan_type_labels):\n",
    "        if scan_type in width_height_data:\n",
    "            widths, heights = width_height_data[scan_type]\n",
    "            \n",
    "            # Create 2D histogram\n",
    "            heatmap_data, x_edges, y_edges, im = axes[i].hist2d(\n",
    "                widths, heights, bins=20, cmap='plasma', cmin=1\n",
    "            )\n",
    "            \n",
    "            # Add color bar\n",
    "            cbar = fig.colorbar(im, ax=axes[i])\n",
    "            cbar.set_label('Frequency')\n",
    "            \n",
    "            # Set labels and title\n",
    "            axes[i].set_title(f'{scan_type}')\n",
    "            axes[i].set_xlabel('Width (Columns)')\n",
    "            axes[i].set_ylabel('Height (Rows)')\n",
    "            \n",
    "            # Fine-grain ticks\n",
    "            axes[i].set_xticks(np.linspace(min(widths), max(widths), num=5))\n",
    "            axes[i].set_yticks(np.linspace(min(heights), max(heights), num=5))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_heatmaps_for_width_and_height(dicom_metadata)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each scan has a different size. The T2 scan has two areas of high concentration near the (608, 624) height and width and near the (400, 424) height and width. The T1 scan has the highest concentration between (448, 448) height and width and (640, 640). The STIR scan does not have much data in our data set, so we must consider the width and height shifts that can happen in different data. Also, the T1 scan tends to have a more symmetric width and height for each image, and the T2 scan is not symmetric. We can pad the images of the T2 scan in order to make them symmetric; the padding will not ruin the image much because the T2 scan MRI images are almost symmetric and have a close aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. The histogram of the image depth for each MRI scan.\n",
    "\n",
    "def plot_histogram_of_slices(dicom_count) -> None:\n",
    "    # Prepare data for each scan type\n",
    "    slice_data = {scan_type: list(file_counts) for scan_type, file_counts in dicom_count.items()}\n",
    "\n",
    "    # Set up the figure with subplots for each scan type\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(slice_data), figsize=(15, 6), sharey=True)\n",
    "    fig.suptitle('Histogram of Number of Slices per Series for Each Scan Type')\n",
    "\n",
    "    # Loop through each scan type and create a histogram\n",
    "    for i, (scan_type, slices) in enumerate(slice_data.items()):\n",
    "        # Plot histogram for the current scan type\n",
    "        axes[i].hist(slices, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'{scan_type}')\n",
    "        axes[i].set_xlabel('Number of Slices')\n",
    "        axes[i].set_ylabel('Frequency' if i == 0 else \"\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram_of_slices(dicom_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the T2 scan, most of the data has less than 75 slices, and in the T1 scan, most of the data has less than 30 slices. We don't have enough data to reach a conclusion regarding the STIR scan.\n",
    "Our algorithm must handle a three-dimensional pixel array of different depths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to dive deeper into each disease in the data to really understand the patterns and insights they reveal, allowing us to draw more meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diseases in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will plot each disease from the data to understand how they actually look like in our collection of MRI images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spinal Canal Stenosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinal_canal_stenosis_s = pydicom.dcmread(cfg.SCS_Sagittal).pixel_array\n",
    "spinal_canal_stenosis_s = np.flip(spinal_canal_stenosis_s, axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(spinal_canal_stenosis_s, cmap='gray')\n",
    "ellipse = patches.Ellipse((125, 48), width=45, height=40, edgecolor='red', facecolor='none', linewidth=2)\n",
    "plt.gca().add_patch(ellipse)\n",
    "plt.title(\"Sagittal - Spinal Canal Stenosis L1/L2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bump marked with an ellipse is in the L1-L2 area. The bump narrows the spine canal, which is why we need to look for such bumps to locate Spinal Canal Stenosis disease.\n",
    "For this disease, there is value also in the Axial scan so that we will show it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinal_canal_stenosis_a = pydicom.dcmread(cfg.SCS_AXIAL).pixel_array\n",
    "spinal_canal_stenosis_a = np.flip(spinal_canal_stenosis_a, axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(spinal_canal_stenosis_a, cmap='gray')\n",
    "ellipse = patches.Ellipse((170, 278), width=35, height=60, edgecolor='red', facecolor='none', linewidth=2)\n",
    "plt.gca().add_patch(ellipse)\n",
    "plt.title(\"Axial - Spinal Canal Stenosis L1/L2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The narrowing spine canal is highlighted in the MRI image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Foraminal Narrowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Foraminal Narrowing is harder to spot. We want to look for the abstance of fat near the root using a T1-Weighted scan ( Saggital ).\n",
    "The following example will show the left side, but the same is true but the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_foraminal_narrowing_s = pydicom.dcmread(cfg.NFA_Sagittal).pixel_array\n",
    "neural_foraminal_narrowing_s = np.flip(neural_foraminal_narrowing_s, axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(neural_foraminal_narrowing_s, cmap='gray')\n",
    "ellipse = patches.Ellipse((169, 58), width=20, height=20, edgecolor='red', facecolor='none', linewidth=2)\n",
    "plt.gca().add_patch(ellipse)\n",
    "plt.title(\"Saggital - Left Neural Foraminal Narrowing L1/L2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The T1-Weighted scan highlights the fat in the body. If we see a dark area near the root, then that is a sign of Neural Foraminal Narrowing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subarticular Stenosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarticular_stenosis_a = pydicom.dcmread(cfg.SS_AXIAL).pixel_array\n",
    "subarticular_stenosis_a = np.flip(subarticular_stenosis_a, axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(subarticular_stenosis_a, cmap='gray')\n",
    "ellipse = patches.Ellipse((182, 210), width=20, height=20, edgecolor='red', facecolor='none', linewidth=2)\n",
    "plt.gca().add_patch(ellipse)\n",
    "plt.title(\"Axial - Left Subarticular Stenosis L1/L2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This disease can be spotted by looking for bumps in the lateral recess area from the left or right side. The bumps can come from up or down to the lateral recess. In our example, we can spot a bump in the left area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRI Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will analyze the properties of MRI images.  \n",
    "If a property has already been extracted from the DICOM metadata and is directly available in the image, analysis will be skipped for that property.  \n",
    "  \n",
    "We will begin by loading the image data.  \n",
    "The data from the functions extract_edges, compute_texture_features and compute_sharpness will be used and explained later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.image_dataloader import load_batches\n",
    "\n",
    "k = 100\n",
    "num_workers = 4\n",
    "percent_total_batch_size = 0.01 # It's not a good idea to load more than 30gb of images data\n",
    "\n",
    "image_dict = defaultdict(list)\n",
    "\n",
    "def store_images(batch_data: List[Dict[str, List[np.ndarray]]]) -> None:\n",
    "    \"\"\"\n",
    "    Store images first before processing them.\n",
    "    \"\"\"\n",
    "    for batch_dict in batch_data:\n",
    "        for condition, image_list in batch_dict.items():\n",
    "            # Normalize, scale, and store\n",
    "            image_dict[condition].extend(\n",
    "                [np.floor((image - image.min()) / (image.max() - image.min() + 1e-8) * 255).astype(np.uint8)\n",
    "                 for image in image_list]\n",
    "            )\n",
    "\n",
    "# Load images into the dictionary\n",
    "load_batches(cfg.TRAIN_IMAGES_PATH, cfg.TRAIN_LABEL_COORDINATES_CSV_PATH, k, store_images, num_workers, percent=percent_total_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed feature storage\n",
    "brightness_values_per_condition = defaultdict(set)\n",
    "contrast_values_per_condition = defaultdict(set)\n",
    "edges_per_condition = defaultdict(list)\n",
    "sharpness_values_per_condition = defaultdict(list)\n",
    "\n",
    "def extract_edges(image: np.ndarray) -> np.ndarray:\n",
    "    return cv2.Canny(image, threshold1=50, threshold2=150)\n",
    "\n",
    "def compute_sharpness(image: np.ndarray) -> float:\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def process_single_image(image: np.ndarray) -> Tuple[np.uint8, np.uint8, np.ndarray, float]:\n",
    "    \"\"\"Processes a single image and returns computed features.\"\"\"\n",
    "    brightness = np.uint8(image.mean())\n",
    "    contrast = np.uint8(image.std())\n",
    "    edges = extract_edges(image)\n",
    "    sharpness = compute_sharpness(image)\n",
    "    return brightness, contrast, edges, sharpness\n",
    "\n",
    "def process_images_batch(condition: str, images: List[np.ndarray], progress_bar: tqdm) -> None:\n",
    "    \"\"\"\n",
    "    Processes a batch of images for a given condition.\n",
    "    \"\"\"\n",
    "    if not images:\n",
    "        return  # Skip empty conditions\n",
    "\n",
    "    results = []\n",
    "    for image in images:\n",
    "        results.append(process_single_image(image))\n",
    "        progress_bar.update(1)  # Update progress after processing each image\n",
    "\n",
    "    # Store features per condition\n",
    "    for brightness, contrast, edges, sharpness in results:\n",
    "        brightness_values_per_condition[condition].add(brightness)\n",
    "        contrast_values_per_condition[condition].add(contrast)\n",
    "        edges_per_condition[condition].append(edges)\n",
    "        sharpness_values_per_condition[condition].append(sharpness)\n",
    "\n",
    "def process_all_images() -> None:\n",
    "    \"\"\"\n",
    "    Processes images in parallel after all data has been loaded\n",
    "    \"\"\"\n",
    "    total_images = sum(len(images) for images in image_dict.values())\n",
    "\n",
    "    with tqdm(total=total_images, desc=\"Processing Images\", unit=\"image\") as progress_bar:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(process_images_batch, condition, images, progress_bar): condition \n",
    "                       for condition, images in image_dict.items()}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # Ensure all batches are processed\n",
    "\n",
    "\n",
    "process_all_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contrast and brightness property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the graphs first. After that, we'll explain what they show us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge left/right conditions\n",
    "def merge_conditions(brightness_dict: Dict[str, List[int]]) -> Dict[str, List[int]]:\n",
    "    merged_data = defaultdict(list)\n",
    "\n",
    "    for condition, values in brightness_dict.items():\n",
    "        base_condition = re.sub(r'\\b(left|right)\\b', '', condition, flags=re.IGNORECASE).strip()\n",
    "        merged_data[base_condition].extend(values)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "merged_brightness = merge_conditions(brightness_values_per_condition)\n",
    "merged_contrast = merge_conditions(contrast_values_per_condition)\n",
    "\n",
    "num_bins = 15\n",
    "bins = np.linspace(0, 255, num_bins)\n",
    "\n",
    "brightness_heatmap_data = []\n",
    "contrast_heatmap_data = []\n",
    "condition_labels = []\n",
    "\n",
    "for condition in merged_brightness.keys():\n",
    "    # Process Brightness\n",
    "    brightness_array = np.clip(np.array(merged_brightness[condition]), 0, 255)\n",
    "    brightness_hist, _ = np.histogram(brightness_array, bins=bins)\n",
    "    brightness_heatmap_data.append(brightness_hist)\n",
    "\n",
    "    # Process Contrast\n",
    "    contrast_array = np.clip(np.array(merged_contrast[condition]), 0, 255)\n",
    "    contrast_hist, _ = np.histogram(contrast_array, bins=bins)\n",
    "    contrast_heatmap_data.append(contrast_hist)\n",
    "\n",
    "    condition_labels.append(condition)\n",
    "\n",
    "# Convert lists to NumPy arrays for plotting\n",
    "brightness_heatmap_data = np.array(brightness_heatmap_data)\n",
    "contrast_heatmap_data = np.array(contrast_heatmap_data)\n",
    "\n",
    "# Set up subplots for dual heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Brightness Heatmap\n",
    "sns.heatmap(\n",
    "    brightness_heatmap_data, \n",
    "    cmap=\"coolwarm\", \n",
    "    xticklabels=np.round(bins).astype(int), \n",
    "    yticklabels=condition_labels, \n",
    "    ax=axes[0], \n",
    "    cbar=True\n",
    ")\n",
    "axes[0].set_title(\"Brightness Distribution Heatmap\")\n",
    "axes[0].set_xlabel(\"Brightness (Mean Pixel Value)\")\n",
    "axes[0].set_ylabel(\"Condition\")\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Contrast Heatmap\n",
    "sns.heatmap(\n",
    "    contrast_heatmap_data, \n",
    "    cmap=\"viridis\",  # Use a different color map for contrast\n",
    "    xticklabels=np.round(bins).astype(int), \n",
    "    yticklabels=condition_labels, \n",
    "    ax=axes[1], \n",
    "    cbar=True\n",
    ")\n",
    "axes[1].set_title(\"Contrast Distribution Heatmap\")\n",
    "axes[1].set_xlabel(\"Contrast (Standard Deviation)\")\n",
    "axes[1].set_ylabel(\"Condition\")\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a heat map for the brightness and contrast.  \n",
    "The pins below show the value and the color shows the amount of MRI scans in the value. On the right, you can see the color distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "brightness_flat = []\n",
    "contrast_flat = []\n",
    "\n",
    "# Ensure both lists are the same length\n",
    "for condition in brightness_values_per_condition.keys():\n",
    "    brightness_values = list(brightness_values_per_condition[condition])\n",
    "    contrast_values = list(contrast_values_per_condition.get(condition, []))  # Handle missing conditions\n",
    "    \n",
    "    # Take only the minimum length to avoid mismatched pairs\n",
    "    min_length = min(len(brightness_values), len(contrast_values))\n",
    "    \n",
    "    if min_length > 0:\n",
    "        brightness_flat.extend(brightness_values[:min_length])\n",
    "        contrast_flat.extend(contrast_values[:min_length])\n",
    "\n",
    "\n",
    "brightness_flat = np.array(brightness_flat)\n",
    "contrast_flat = np.array(contrast_flat)\n",
    "\n",
    "# Ensure both arrays are non-empty and have at least 2 values\n",
    "if len(brightness_flat) > 1 and len(contrast_flat) > 1:\n",
    "    # Compute Pearson correlation coefficient\n",
    "    correlation, p_value = stats.pearsonr(brightness_flat, contrast_flat)\n",
    "\n",
    "    # Scatter plot to visualize correlation\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(brightness_flat, contrast_flat, alpha=0.5)\n",
    "    plt.xlabel(\"Brightness (Mean Pixel Value)\")\n",
    "    plt.ylabel(\"Contrast (Standard Deviation)\")\n",
    "    plt.title(\"Brightness vs Contrast\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Brightness-Contrast Correlation: r = {correlation:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough data to compute Pearson correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In medical imaging, brightness and contrast are fundamental properties that influence the quality and interpretability of scans. These properties affect how well clinicians, researchers, and machine learning models can detect abnormalities and distinguish between different conditions.\n",
    "\n",
    "Brightness refers to the overall intensity of an image, representing how light or dark it appears. A high-brightness image might indicate overexposure, potentially obscuring important details, while a low-brightness image might be too dark, making it difficult to identify structures. Standardizing brightness helps ensure consistency across different scans, improving diagnostic accuracy. Brightness is commonly measured as the mean pixel intensity:\n",
    "\n",
    "$$\n",
    "\\text{Brightness} = \\frac{1}{N} \\sum_{i=1}^{N} I_i\n",
    "$$\n",
    "\n",
    "where $ I_i $ represents pixel intensity values, typically ranging from 0 to 255 for grayscale images.\n",
    "\n",
    "Contrast refers to the difference in intensity between different regions of an image. It determines how well structural details stand out. Low contrast images make it difficult to distinguish between tissues, leading to potential misdiagnoses, while high contrast images improve visibility, making anomalies easier to detect. Certain conditions, such as tumors or fractures, become more apparent in high-contrast regions. Contrast is often measured using the standard deviation of pixel intensities:\n",
    "\n",
    "$$\n",
    "\\text{Contrast} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (I_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "where $ \\mu $ is the mean pixel intensity. A higher standard deviation indicates greater variation in pixel intensities, signifying higher contrast.\n",
    "\n",
    "The correlation between brightness and contrast provides insight into how these two properties interact in medical images. Key questions include whether increasing brightness reduces contrast, whether brighter images tend to retain structural detail or become washed out, and whether problematic scans can be identified where extreme brightness or low contrast makes diagnosis difficult. Pearson’s correlation coefficient $ r $ quantifies the relationship between brightness and contrast:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum (B_i - \\bar{B}) (C_i - \\bar{C})}{\\sqrt{\\sum (B_i - \\bar{B})^2 \\sum (C_i - \\bar{C})^2}}\n",
    "$$\n",
    "\n",
    "where $ B_i $ represents the brightness of image $ i $ and $ C_i $ represents the contrast of image $ i $.  \n",
    "If $ r > 0 $, higher brightness is associated with higher contrast.   \n",
    "If $ r < 0 $, higher brightness is associated with lower contrast.   \n",
    "If $ r \\approx 0 $, there is no significant relationship between brightness and contrast.\n",
    "\n",
    "Understanding brightness and contrast is essential in medical diagnostics, ensuring optimal image quality for accurate diagnosis, identifying exposure inconsistencies across different scans, and improving pre-processing techniques for automated analysis. In machine learning applications, training models on well-balanced images enhances diagnostic accuracy, prevents bias in classification by avoiding overrepresentation of brighter or darker images, and enables automated correction of suboptimal scans to enhance consistency.\n",
    "\n",
    "By analyzing their correlation, it is possible to identify inconsistencies in scans, enhance image quality before processing, and improve the performance of machine learning models in diagnostic applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the sharpness of each image and the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_edges = merge_conditions(edges_per_condition)\n",
    "merged_sharpness = merge_conditions(sharpness_values_per_condition)\n",
    "\n",
    "def print_formatted_data() -> pd.DataFrame:\n",
    "    data_rows = []\n",
    "    \n",
    "    for condition in merged_brightness.keys():\n",
    "        mean_sharpness = np.mean(merged_sharpness[condition]) if merged_sharpness[condition] else 0\n",
    "\n",
    "        # Count the number of edge-detected images\n",
    "        edge_count = len(merged_edges[condition])\n",
    "\n",
    "        data_rows.append([\n",
    "            condition,\n",
    "            edge_count,\n",
    "            round(mean_sharpness, 2),\n",
    "        ])\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=[\n",
    "        \"Condition\",\n",
    "        \"Edge Image Count\",\n",
    "        \"Avg Sharpness\"\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "print_formatted_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is valuable for understanding feature importance, class distribution, and model interpretability.  \n",
    "Edge Image Count and Avg Sharpness highlight structural differences between conditions, which can improve classification performance.  \n",
    "The image counts help detect class imbalances, while sharpness variations may serve as meaningful biomarkers.  \n",
    "Additionally, this information guides preprocessing strategies, ensuring consistent image quality for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discussion will focus on the algorithmic approach to solving the problem of predicting the severity of spinal diseases from MRI images. We will present our steps in our solution and challenges we faced in a continuous manner, for a better understanding of the process and the final solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to approach this problem, given that the end result is to predict the severity of the disease in the MRI images. We can start naivly, thinking of the problem as a classification problem, where we can use classical machine learning algorithms such as Random Forest, SVM, or even finding hand-crafted features of the different diseases in each MRI sequence and using them in classification. However, since the amount of data is not too scarce, and we are not real radiologists that specialize in diagnosing spinal diseases, we can use a more advanced approach (and easier to implement) by using deep learning.\n",
    "\n",
    "The next following discussions and code assume the reader has a basic understanding of deep learning and neural networks. If you are not familiar with these concepts, we recommend reading about them before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gain some insights from the EDAs we have done so far:\n",
    "1. The size of the MRI images varies, and the number of slices in each MRI scan is different. This will require us to perform some kind of padding or resizing to make the images uniform.\n",
    "2. The location of the diseases in the MRI images is represented by the x and y coordinates, which we have found to have a high variance. This means that the location of the diseases is not consistent across all images, and we will need to account for this variability in our model. This means that our algorithm will need to either specialize in localizing the disease in the MRI images or to be able to find disease patterns in the MRI images without relying on a priori knowledge of the disease location.\n",
    "3. Some diseases MRI are sharper and have more edges than others\n",
    "4. There is a correlation between the brightness and the contrast in each MRI scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into more advanced non-linear models, we want to see how a traditional model fill fare on our task.  \n",
    "However, traditional models are expensive in run-time, so we will run the model on the batch we loaded in the EDA.  \n",
    "  \n",
    "We'll use the following metric for comparing the models:\n",
    "- **Precision**: Measures how many of the predicted positive cases were actually correct. High precision means fewer false positives.  \n",
    "- **Recall (Sensitivity)**: Indicates how many actual positive cases were correctly identified. High recall means fewer false negatives.  \n",
    "- **F1-score**: The harmonic mean of precision and recall, balancing false positives and false negatives. A higher F1-score indicates better overall classification performance.  \n",
    "- **Support**: The number of actual occurrences of each class in the dataset, providing context for the other metrics.  \n",
    "- **Macro average**: Calculates the average of precision, recall, and F1-score across all classes, treating them equally regardless of class size.  \n",
    "- **Weighted average**: Similar to the macro average but accounts for class imbalance by weighting scores based on the number of samples in each class.  \n",
    "- **Accuracy**: The overall percentage of correctly classified samples, but it may be misleading when dealing with imbalanced data.  \n",
    "  \n",
    "The classification report helps analyze a model’s strengths, weaknesses, and biases, guiding improvements for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Why SVC with Feature Extraction, KNN, and Logistic Regression Are Good Choices for MRI Classification</strong>  \n",
    "\n",
    "Using Support Vector Classifier (SVC) with feature extraction, K-Nearest Neighbors (KNN), and Logistic Regression separately for MRI classification is a strong approach because each model offers unique advantages suited to medical imaging. MRI scans are high-dimensional, complex, and often contain subtle variations that require different classification techniques depending on the dataset and problem at hand. Each model has characteristics that make it well-suited for analyzing MRI images individually.  \n",
    "\n",
    "<strong>Support Vector Classifier (SVC) with Feature Extraction</strong>  \n",
    "SVC is particularly effective for <strong>high-dimensional</strong> data, making it a strong choice for MRI classification. Medical images contain vast amounts of information, much of which may be redundant or irrelevant for classification. <strong>Feature extraction</strong> reduces this complexity by transforming the MRI scans into a lower-dimensional space that highlights the most relevant patterns, such as texture, edge information, or shape characteristics. This makes SVC more efficient and reduces the risk of overfitting.  \n",
    "\n",
    "Since feature extraction maps the MRI data into a lower-dimensional feature space, SVC typically works best with a <strong>linear kernel</strong>. The transformation step ensures that the important patterns are captured in a way that makes the data linearly separable, allowing SVC to classify cases efficiently. A linear kernel is computationally faster and reduces the risk of overfitting compared to non-linear kernels, making it a practical choice when using feature extraction techniques.  \n",
    "\n",
    "<strong>K-Nearest Neighbors (KNN) on Raw MRI Images</strong>  \n",
    "KNN is a <strong>non-parametric algorithm</strong> that classifies data based on similarity, making it a natural choice for MRI scans, where structural patterns and intensity distributions are key indicators of medical conditions. Since KNN does not require training in the traditional sense, it can adapt well to complex datasets without making assumptions about the underlying data distribution.  \n",
    "\n",
    "This is particularly useful for MRI images, where the relationships between different scans may not be easily captured by a fixed mathematical function. KNN is also <strong>highly interpretable</strong>, as its predictions are based on the most similar existing examples, making it useful in medical settings where explainability is important. While KNN can be computationally expensive for large datasets, it performs well when working with smaller, well-structured MRI datasets, especially when combined with distance metrics that are effective for image data, such as Euclidean or cosine similarity.  \n",
    "\n",
    "<strong>Logistic Regression on Raw MRI Images</strong>  \n",
    "Logistic Regression is a <strong>simple yet effective model</strong> for classification that can handle **multi-class problems**, making it a suitable choice for MRI scans where there are **three different classes**. It extends naturally from binary classification to multi-class settings using techniques like **one-vs-rest (OvR) or softmax regression**, allowing it to separate multiple categories efficiently.  \n",
    "\n",
    "It models the relationship between pixel values (or extracted image features) and the probability of a particular class using a <strong>logistic function</strong>, which outputs values between 0 and 1. This makes it useful for medical diagnosis, as it provides <strong>confidence scores</strong> for predictions, which can be valuable for clinical decision-making. Additionally, Logistic Regression is <strong>computationally efficient</strong> and less prone to overfitting compared to more complex models, especially when working with a limited number of MRI samples. Because of its interpretability and reliability, it serves as a solid baseline model for MRI classification before exploring more complex approaches.  \n",
    "\n",
    "<strong>Why These Models Are a Good Choice for MRI Classification</strong>  \n",
    "Each of these models is well-suited for MRI image classification for different reasons:  \n",
    "<ul>\n",
    "  <li><strong>SVC with feature extraction</strong> excels in capturing <strong>complex patterns and decision boundaries</strong> while reducing dimensionality and ensuring linear separability.</li>\n",
    "  <li><strong>KNN</strong> is effective for MRI scans due to its <strong>instance-based learning</strong>, which preserves structural similarities and adapts well to new data.</li>\n",
    "  <li><strong>Logistic Regression</strong> is a reliable choice for <strong>multi-class classification</strong> in medical imaging, particularly for datasets with three distinct categories.</li>\n",
    "</ul>  \n",
    "\n",
    "Since MRI datasets can vary widely in terms of size, noise, and structure, selecting the best model depends on the specific characteristics of the dataset. However, each of these models provides a strong foundation for medical image classification, making them good choices for analyzing MRI scans individually.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use a feature extraction model.  \n",
    "  \n",
    "After testing, we found that a polynomial kernel of degree 3 works well and has good flexibility.  \n",
    "C is the regularization parameter ( L2 regularization ), \"C=6.0\" is a good choice because we don't want to penalize our model too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Ensure all feature lists have the same length for each condition\n",
    "for condition in merged_brightness.keys():\n",
    "    if (condition in merged_contrast and condition in merged_edges and condition in merged_sharpness):\n",
    "        brightness_vals = merged_brightness[condition]\n",
    "        contrast_vals = merged_contrast[condition]\n",
    "        sharpness_vals = merged_sharpness[condition]\n",
    "\n",
    "        for b, c, s in zip_longest(brightness_vals, contrast_vals, sharpness_vals, fillvalue=None):\n",
    "            if None not in (b, c, s):  # Ensure all features exist for this image\n",
    "                feature_vector = [float(b), float(c), float(s)]\n",
    "                X.append(feature_vector)\n",
    "                y.append(condition)  # Condition as label\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel='poly', C=6.0, degree = 3, random_state=42, probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report highlights significant issues in the model’s ability to distinguish between the three MRI-based conditions, as evidenced by its low precision, recall, and F1-scores. Neural Foraminal Narrowing suffers the most, with a recall of only 33%, meaning the model correctly identifies less than one-third of actual cases while misclassifying the majority. Spinal Canal Stenosis achieves the highest recall (73%), but at the cost of low precision (44%), indicating that a large proportion of its positive predictions are actually incorrect. This suggests the model is over-predicting this class while failing to distinguish it properly from the others. Subarticular Stenosis, though performing relatively better, still has only 66% precision and 63% recall, resulting in an F1-score of 0.64, which remains suboptimal for medical applications. The overall accuracy of 56% is only slightly above random guessing in a three-class setting, demonstrating that the model lacks strong discriminatory power. The macro average F1-score of 0.54 and weighted average F1-score of 0.55 further confirm that the poor performance is not limited to a single class but affects all predictions. The imbalance between precision and recall across classes suggests that the model is neither confident in its classifications nor consistently capturing the key patterns needed for reliable MRI diagnosis. Given that medical imaging requires high classification accuracy to support clinical decisions, these results indicate that the model is currently inadequate for practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how a K-Nearest Neighbors model will fare.  \n",
    "\n",
    "We have alot of MRI images, so we want to choose K ( number of neighbors ) such that there will be no underfitting or overfitting.  \n",
    "A common choice is K = 11-15 for large databases, so we will choose K = 11.  \n",
    "For the weights and metric we picked the standard choice for most models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_list, img_size: Tuple = (180, 180)) -> np.ndarray:\n",
    "    \"\"\" Pads and resizes images to a fixed shape \"\"\"\n",
    "    if not image_list:\n",
    "        return np.array([])\n",
    "\n",
    "    processed_images = []\n",
    "    for img in image_list:\n",
    "        if isinstance(img, np.ndarray):\n",
    "            resized_img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "            processed_images.append(resized_img.flatten())  # Flatten after resizing\n",
    "\n",
    "    return np.array(processed_images, dtype=np.float32)\n",
    "\n",
    "# Merge conditions for images\n",
    "merged_images = merge_conditions(image_dict)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "epochs = 100\n",
    "image_size = (360, 360)\n",
    "\n",
    "for condition, images in merged_images.items():\n",
    "    processed_images = preprocess_images(images, img_size=image_size)  # Resize before training\n",
    "    if processed_images.size > 0:\n",
    "        X.extend(processed_images)  # Append all processed images\n",
    "        y.extend([condition] * len(processed_images))  # Append corresponding labels\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X = np.stack(X)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "k_neighbors = 11\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_neighbors, weights=\"uniform\", metric=\"euclidean\")\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the high overall accuracy of 83%, this classification report still highlights weaknesses that make the model unreliable, particularly for medical diagnosis. One major issue is the imbalance in class performance, where Subarticular Stenosis is classified with near-perfect precision (99%) and recall (96%), while the other two conditions, Neural Foraminal Narrowing and Spinal Canal Stenosis, perform significantly worse. The precision of 66% and recall of 58% for Spinal Canal Stenosis indicates that over 40% of actual cases are misclassified, meaning many affected patients could go undetected. Similarly, while Neural Foraminal Narrowing has a better recall of 75%, its precision is only 63%, leading to a relatively low F1-score of 0.68, which suggests that a substantial number of incorrect predictions are being made.\n",
    "\n",
    "The macro average F1-score of 0.76 reveals that, on average, the model does not perform consistently well across all classes. The weighted averages (0.83 across the board) are inflated by the dominance of Subarticular Stenosis, which has the highest support (244 cases out of 442 total). This suggests that the model is biased toward the most common class and struggles with distinguishing between the less frequent conditions. The poor recall for Spinal Canal Stenosis (58%) is particularly concerning in a medical context, as it means that over 40% of actual cases are missed, potentially leading to undiagnosed or untreated conditions.\n",
    "\n",
    "In summary, while the high accuracy may seem impressive, the imbalance in precision and recall across classes indicates that the model is overfitting to Subarticular Stenosis while failing to reliably classify the other conditions. This makes it unsuitable for real-world medical use, where misclassification could lead to serious consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check if a logistical regression model is good for our task.  \n",
    "Logistical regression can handle imbalanced data better and is more robust than KNN, so it's a good idea compare it to KNN for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_list, img_size: Tuple = (180, 180)) -> np.ndarray:\n",
    "    \"\"\" Pads and resizes images to a fixed shape \"\"\"\n",
    "    if not image_list:\n",
    "        return np.array([])\n",
    "\n",
    "    processed_images = []\n",
    "    for img in image_list:\n",
    "        if isinstance(img, np.ndarray):\n",
    "            resized_img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "            processed_images.append(resized_img.flatten())  # Flatten after resizing\n",
    "\n",
    "    return np.array(processed_images, dtype=np.float32)\n",
    "\n",
    "# Merge conditions for images\n",
    "merged_images = merge_conditions(image_dict)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "epochs = 100\n",
    "image_size = (260, 260)\n",
    "\n",
    "for condition, images in merged_images.items():\n",
    "    processed_images = preprocess_images(images, img_size=image_size)\n",
    "    \n",
    "    if processed_images.size > 0:\n",
    "        X.extend(processed_images)  # Append all processed images\n",
    "        y.extend([condition] * len(processed_images))  # Append corresponding labels\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X = np.stack(X)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "log_reg_model = LogisticRegression(max_iter=epochs, solver='saga', random_state=42)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report for this Logistic Regression model shows an 84% overall accuracy, which may seem strong, but a deeper analysis reveals significant issues in class balance, predictive reliability, and convergence. The model exhibits a clear performance disparity between the three classes. Neural Foraminal Narrowing achieves 75% recall, meaning it correctly identifies most true cases, but its 65% precision indicates that 35% of its predictions are incorrect, leading to an F1-score of 0.70. Similarly, Spinal Canal Stenosis, with 69% precision and 60% recall, struggles to consistently detect cases, missing 40% of actual occurrences, resulting in an F1-score of 0.64. Meanwhile, Subarticular Stenosis heavily dominates the classification, with an extremely high 98% precision and 97% recall, producing an F1-score of 0.97. The model appears biased towards the most frequent class (Subarticular Stenosis) while failing to reliably distinguish between the other two, which is problematic in medical applications where misclassification can have serious consequences.\n",
    "\n",
    "The macro average F1-score of 0.77 suggests an imbalance in classification quality, and the weighted average of 0.84 is likely inflated by the overwhelming number of correctly classified Subarticular Stenosis cases. While the model performs well in distinguishing that condition, it fails to generalize well across all three classes, particularly Spinal Canal Stenosis, where recall remains weak (60%), meaning that 40% of affected patients would not be identified.\n",
    "\n",
    "Beyond classification performance, the convergence warning is a critical issue that directly impacts model reliability. The message \"max_iter was reached which means the coef_ did not converge\" suggests that the Logistic Regression solver failed to fully optimize the model weights before training stopped. This usually occurs when the dataset is complex, linearly inseparable, or when the solver requires more iterations to properly adjust coefficients. In this case, the failure to converge means the model's decision boundaries are suboptimal, leading to unstable predictions and reduced generalization capability. Without proper convergence, the model is likely making erratic weight adjustments, which can manifest as overconfidence in one class (Subarticular Stenosis) and poor discrimination between the others. The presence of this warning means that the model's reported accuracy and performance metrics cannot be fully trusted, as the coefficients have not settled into their optimal values.\n",
    "\n",
    "In summary, while the overall accuracy appears high, the uneven performance across classes, poor recall for Spinal Canal Stenosis, and failure to converge indicate that this Logistic Regression model is not fully optimized and remains unreliable for medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the classic **feature extraction + SVC model**, the **Logistic Regression model**, and the **K-Nearest Neighbors (KNN) model** fail to handle our MRI data effectively, as evidenced by their poor classification performance, optimization issues, and inability to generalize across all conditions.  \n",
    "\n",
    "- **Feature extraction models** (e.g., SVC with feature extraction) are **computationally expensive**, require significant memory, and struggle with **capturing complex spatial features** in MRI images. Their reliance on **linear separability** prevents them from distinguishing between classes with overlapping characteristics, leading to **low recall (as low as 33%) and an overall accuracy of only 56%**, barely above random chance. This results in unreliable predictions that are unsuitable for medical diagnosis.  \n",
    "\n",
    "- **Logistic Regression models** suffer from **high memory usage and convergence failure**, as seen in the **max_iter reached warning**, which indicates that the model’s coefficients did not fully optimize. This leads to **unstable decision boundaries and poor generalization**, particularly affecting **Spinal Canal Stenosis**, where recall remains low (**60%**). The model is heavily **biased towards Subarticular Stenosis**, with an inflated precision (**98%**) and recall (**97%**), making it ineffective for handling class imbalances in MRI data.  \n",
    "\n",
    "- **K-Nearest Neighbors (KNN)** struggles with **high-dimensional MRI data**, as **distance-based similarity becomes unreliable** in complex feature spaces. Additionally, **KNN is highly sensitive to class imbalances**, leading to **poor recall for underrepresented conditions**, such as Spinal Canal Stenosis (**58% recall**), which could result in a large number of undiagnosed cases. While the model achieves a seemingly high **accuracy of 83%**, this is misleading, as it heavily overfits to the dominant class (**Subarticular Stenosis**), failing to provide balanced classification across all conditions.  \n",
    "\n",
    "Given these limitations, a **Deep Learning Model is necessary**. Unlike traditional methods, deep learning can automatically extract hierarchical features, capture non-linear relationships, and scale effectively with large datasets. Neural networks, particularly **Convolutional Neural Networks (CNNs)**, are well-suited for MRI analysis as they can learn spatial structures, texture patterns, and anatomical variations without requiring manual feature engineering. Given the complexity of MRI classification and the inability of traditional models to handle class imbalances, capture complex patterns, and properly optimize decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few solutions we have thought of to tackle this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROI Detection and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, one common approach to handling medical image data is to use a two-stage approach: first, detect the region of interest (ROI) in the image, and then classify the ROI to determine the presence and severity of the disease. This approach is intuitive to use for a few positive reasons:\n",
    "1. Reduced complexity of the problem by breaking it down into two simpler tasks.\n",
    "2. Good runtime performance, as the ROI detection can be done quickly and efficiently, and the classification is done on a smaller subset of the image.\n",
    "3. Flexibility in the model architecture, as we can use different models for ROI detection and classification.\n",
    "\n",
    "We also have the $x, y$ coordinates of the disease in the MRI images, which can be used to train a model to detect the ROI. After some research, we have also found out about Meta's Segmant Segment Anything Model 2 ([SAM 2](https://ai.meta.com/sam2/)). SAM 2 is a deep learning model that can be used to segment any object in an image, and it has been shown to perform well on this task from the competition forum. We can use SAM 2 to detect the ROI in the MRI images and then use a separate model to classify the ROI.\n",
    "\n",
    "All said and done, we have ultimately decided to not use this approach of detecting the ROI and then classifying for a few reasons:\n",
    "\n",
    "1. The ROI detection model will need to be trained on the $x, y$ coordinates of the disease in the MRI images, which is another task in itself. The model will need to generalize well to unseen data, and it may not be able to do so effectively.\n",
    "2. The ROI that will be extracted may exlude some important features of the disease that are not in the $x, y$ coordinates. This may lead to a loss of information and a decrease in the classification model's performance.\n",
    "3. If the ROI model fails, the classification model will not be able to perform well, as it will not have the necessary information to classify the disease. This can lead to diverging results and a decrease in the model's performance during inference.\n",
    "4. The implementation of the ROI detection model will require additional time and resources, which may not be feasible given the time constraints of the competition. It will require us to understand the use of the open source SAM 2 model and to train it on our data, which may take a significant amount of time.\n",
    "5. SAM 2 works on 2D data, but our data is 3D. This means we would have to apply the model to each slice of the MRI images, and use some algorithm to combine the results of the model to get the final ROI. This can be complex and may not work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-to-End 3D Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one of our proposed solutions is to use an end-to-end 3D classification model to classify the severity of the disease in the MRI images. This approach involves training a single model to take the entire 3D MRI image as input and output the predicted severity of the disease. This approach handles the 5 problems we have mentioned above with the ROI detection and classification approach, but it introduces a new problem, that the single model will need to be able to handle the 3 different distributions of the MRI images (Sagittal T2/STIR, Sagittal T1, and Axial T2). The obvious solution to this problem is to use a multi-modal model that can handle the different distributions of the MRI images. Meaning, the model will contain 3 different branches, each branch handling a different distribution of the MRI images. The branches will then be combined to output the final prediction. Below is a high-level diagram of the proposed model architecture:\n",
    "\n",
    "<img src=\"notebook images/Algorithmic Approach/multi-modal-1.png\" width=\"100%\" height=\"400px\">\n",
    "\n",
    "This is indeed in high-level the ultimate solution we have chosen to go with, aside from a few tweeks and optimizations we will discuss later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our solution is to handle the data and preprocess it for training. We need a way to load the MRI images with their corresponding labels, and mark each patient as a data point $(x, y)$ This is done in the `MultiModelLumbarSpineDataset` class, please visit the source code in `src.dataset.spine_dataset` for more information.\n",
    "\n",
    "In short, the class loads a patient's data point in its `__getitem__` method, and returns a dictionary with\n",
    "1. `\"data\"`: The MRI images of the patient - list of tensors wit the shape `(D, 1, H, W)`.\n",
    "2. `\"target\"`: The target label of the severity of the diseases in the MRI images - tensor of shape `(25)`.\n",
    "3. `\"series_types\"`: The series types of the MRI images - list of encoded series types.\n",
    "\n",
    "**Note**: As we have seen, there could be variability in the height and width of the MRI images, and it turns out that there could be variability within the same series! This problem was a hassle, but we solved it with padding. At the loading phase, the MRI series is padded with zeroes from all sides to the size largest slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More simple pre-processing is done in the `MultiModelLumbarSpineDataset` class on the CPU, such as converting the MRI images to tensors, and converting dtype to float32. Also, some simple augmentations is done on the CPU as well. We've decided to not do any serious pre-processing on the CPU, and save them to the GPU. For this reason, we couldn't do these pre-processing in the dataset class, since then we wouldn't be able to use PyTorch's multi-threaded data loading. Further pre-processing is done on the GPU, in the model itself. But, to understand the more complex pre-processing, we need to understand the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the multi-modal model that we have discussed - now we need to choose specific architectures for each branch of the model. After some experimentation, we have decided to use the following architectures for each branch:\n",
    "\n",
    "<img src=\"notebook images/Algorithmic Approach/multi-modal-2.png\" width=\"100%\" height=\"400px\">\n",
    "\n",
    "Notice that we have decided to join the branches of the sagittal axis, to reduce some parameters and save computational power. Ultimetly, the distribution of the T1 and T2/STIR sagittal scans are similar to each other, so a single model can handle them. The axial T2 scan is much different, so we have decided to use a separate model for it. We've also experimented with multiple settings and other architectures, but this one has shown the best results. To view other architectures and settings, please visit the source code in the class `src.model.spine_cnn.LumbarSpineStenosisResNet`. Note that originally, we tried to classify the severities end-to-end from a single series, and we used this class to do so. After deciding to use the multi-modal model, we kept using this class as a branch in the multi-modal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some more technical holes to fill in. First, we need to discuss the aggregation of the feature vectors from each branch. Each branch outputs a list of feature tensors that corresponds with the series types of the MRI images. For example, if patient A has a single sagittal T1 series and two saggital T2/STIR series, the sagittal branch will output 3 feature tensors. We will aggregate those with a simple average pooling. Notice that if there are no MRI images of a specific series type, the branch will output a tensor of zeros. This is important to note, as we will need to handle this in the aggregation. Next up, we need to aggregate the feature vectors from each branch to a single feature vector. We will use a concatenation of the feature vectors, and then pass them through a Multi Layer Perceptron ([MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)) to output the final prediction.\n",
    "We use here an MLP with the following architecture: \n",
    "\n",
    "$$FC_{o \\rightarrow h} \\rightarrow ReLU \\rightarrow Dropout \\rightarrow FC_{h \\rightarrow (25, \\space 3)} \\rightarrow SoftMax_{dim=1}$$\n",
    "\n",
    "Where $FC_{o \\rightarrow h}$ is a fully connected layer from the aggregated feature vector to a hidden layer, $ReLU$ is the Rectified Linear Unit activation function, $Dropout$ is a dropout layer, $FC_{h \\rightarrow (25, \\space 3)}$ is a fully connected layer from the hidden layer to the output layer, and $SoftMax_{dim=1}$ is the SoftMax activation function along the second dimension. The final output is a matrix with 25 rows and 3 columns, where each row corresponds to a disease, and each column corresponds to the probability of the severity of the disease. if no desease is present, the column corrosponding to Normal/Mild will have a high probability, and the other two will have low probabilities.\n",
    "\n",
    "Now that we have understood the architecture of the model, we need to return to the complex data pre-processing. Remember that we have decided to do some of the pre-processing on the GPU, in the model itself. The second pre-processing is done at the beginning of the forward pass of each branch in the model using a python wrapper. Because we have experimented with different architectures, we sometimes even used different transformations for each branch. For the 3DResNet branch we have used the default transformation, including the resizing and and cropping, except for the normalization, which we have set to the mean and variance of the current MRI series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the end-to-end model, we need to create a training loop that will train the model on the training data, and validate it on the validation data. We will use the AdamW optimizer with a cosine annealing learning rate scheduler, and the Cross Entropy Loss function. We've also added loss weights to the loss function, as is done in the competition evaluation (this seemed to also help with the problem of the unbalanced dataset). We will train the model for the maximum time allowed by the competition (9 hours ~ 18 epochs on a P100 GPU), and take the checkpoint with the best validation loss. We will then use this checkpoint to make predictions on the test data and create a submission csv file. Also, due to memory constraints and shape mismatching, we have decided to use a batch size of 1 (meaning the model will update its weights after each patient data point). This is not ideal, but it is the best we can do with the resources we have.\n",
    "\n",
    "To further clarify the training process, the model receives in input the data of a patient containing the MRI seires, the type of the series (the model sends each series type to a different branch), and the target label. The model outputs negative log likelihoods for each disease (25 diseases $\\times$ 3 classes make up 75 NLLs), and the loss function calculates the loss based on the target label and those NLLs, followed by taking a mean over the 25 loss values, similarly to operating on a batch of data. The optimizer then updates the weights of the model based on the loss, and the process is repeated for each batch of data. The model is trained for a number of epochs, and the weights that minimize the loss on the validation data are saved.\n",
    "\n",
    "For inference, we will load the saved model checkpoint, and use it to make predictions on the test data. To make a prediction, we will pass (similarly to the training process) the MRI data and series type of a patient to the model, and the model will output NLLs for each disease. It is the job of the user to convert these NLLs to probabilities if needed. To extract a prediction, the argmax of the NLLs (per desease) is taken, and the corresponding severity (again, per desease), totalling 25 predictions is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have experimented with different model architectures, hyperparameters, data pre-processing and augmentation techniques to find the best model for the task at hand. We managed to find improvements in the model's performance after almost each experiment, and our some of our conclusions are:\n",
    "1. Using pre-trained models on this problem helped with the converges speed, but the final performance was not better than training from scratch. We think probably since the domain of the pre-trained models was so much different from lumbar spine domain. Of course, these pre-trained models came from HuggingFace\\PyTorch and we used transfer learning to train them on our data.\n",
    "2. Using the multi-modal model was a good choice, as it helped with the performance of the model. We have experimented with different architectures for the model, and the one we have chosen (and already presented) has shown the best results.\n",
    "3. Normalizing the MRI images to the mean and variance of the current MRI series has shown to improve the performance of the model. This is probably because the MRI images have different distributions, and normalizing them to the mean and variance of the current MRI series helps the model to learn the features of the MRI images better. We also had many problems regarding divergence of the model, and most of them arrised from faulty normalization.\n",
    "4. Using the AdamW optimizer with a cosine annealing learning rate scheduler has shown to improve the performance of the model. We have experimented with different optimizers and learning rate schedulers, and the AdamW optimizer with a cosine annealing learning rate scheduler were the best combination.\n",
    "5. The loss weights have shown to improve the generalization of the model regarding the scarse classes (Moderate and Severe severity).\n",
    "6. Regularization techniques such as dropout and weight decay have shown to improve the performance of the model significantly.\n",
    "\n",
    "Let the reader understand that we have made almost countless experiments, and if we would show all of them here, this notebook would be too long. We have decided to show only the most successful experiment.\n",
    "\n",
    "Also, keeping in track of all the experiments proved to be a challenge as well, so to keep track of all the experiments, we have used OneDrive to store the checkpoints, run script and logs of each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run script of the best experiment, including the hyperparameters, data pre-processing architecture of the model, and the creation of the submission file, is in the next code block.\n",
    "\n",
    "<style>\n",
    ".note-box {\n",
    "    border-left: 4px solid #0078D4;\n",
    "    border-image: 1;\n",
    "    padding-left: 10px;\n",
    "    border-radius: 0px;\n",
    "    padding-top: 2px;\n",
    "    padding-bottom: 2px;\n",
    "}\n",
    ".note-header {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    padding-bottom: 0.3em;\n",
    "}\n",
    ".note-icon {\n",
    "    font-size: 14px;\n",
    "    color: #0078D4;\n",
    "    background-color: #1E1E1E;\n",
    "    border: 2px solid #0078D4;\n",
    "    border-radius: 100%;\n",
    "    width: 16px;\n",
    "    height: 16px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    margin-right: 8px;\n",
    "}\n",
    ".note {\n",
    "    color: #0078D4;\n",
    "    font-weight: bold;\n",
    "}\n",
    ".note-content {\n",
    "}\n",
    "</style>\n",
    "\n",
    "<blockquote class=\"note-box\">\n",
    " <div class=\"note-header\">\n",
    "  <span class=\"note-icon\">ℹ️</span><strong class=\"note\">Note</strong> \n",
    " </div>\n",
    "  <span class=\"note-content\">The runtime of the next block is the longest in this notebook, it contains the full training process of a model. It takes around 6 Hours on a P100 GPU, please run with consideration.</span>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose num workers parameter.\n",
    "# This affects the number of processes that load the DICOM files in parallel, only increase it if \n",
    "#  you have a strong CPU with sufficient cooling and enough cores.\n",
    "num_workers = 4\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import logging\n",
    "import src.config as cfg\n",
    "from src.model.spine_cnn import MultiModelSpineCNN\n",
    "from src.dataset.spine_dataset import MultiModelLumbarSpineDataset\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Was better without augmentation.\n",
    "dataset = MultiModelLumbarSpineDataset(train=True)#, augs=add_gaussian_noise)\n",
    "train_dataset, val_dataset = dataset.split(val_size=0.15)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "sags_args = dict(architecture=\"R3D_18\", pretrained=False, progress=True, out_features_size=1024)\n",
    "axial_t2_args = dict(architecture=\"R3D_18\", pretrained=False, progress=True, out_features_size=750)\n",
    "model = MultiModelSpineCNN(sags_args, axial_t2_args, last_fc_dim=1024 + 750, dropout=0.5,\n",
    "                            name=\"multi_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you run this in Kaggle.\n",
    "# model.fit(train_loader=train_dataloader, val_loader=val_dataloader, num_epochs=15,\n",
    "#             lr=0.0005, wd=0.0005, try_cuda=True, verbose=True, print_stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our run, we've managed to achieve a stable loss of 0.52 on the validation set and 79.5% accuracy. We used the Kaggle platform and our personal computers for training. But, we could not make a successful submission to the competition, because of an issue with the scoring notebook. This issue was known to the competitors (there are several discussions about it in the competition forum), and we didn't manage to get around it. Therefore, we have decided to evaluate our solution based on the validation loss and accuracy. We trust the validation loss and accuracy as a good indicator of the model's performance since in all our experiments we sample a random validation set from the training data, and by making multiple experiments, we basically did a cross-validation. We are confident that our model would have performed well on the test data, and we are happy with the results we have achieved. See the next figures detailing the training results.\n",
    "\n",
    "<img src=\"notebook images/Algorithmic Approach/training_metrics.png\" width=\"100%\" height=\"400px\">\n",
    "\n",
    "From this training, we took the checkpoint at epoch 8.\n",
    "\n",
    "Please do note that We still produced a submission.csv file, and after reviewing it and the test patient's scan, we happily discovered that the model managed to predict the different diseases (we diagnosed the patient based on our gained knowledge from the EDA). The only downside was that the confidence was not particullary high - around 0.4 for the severe severity. This is probably due to the unbalanced dataset, and the fact that we used random sampling for the mini-batches and did not experiment with any sofisticated sampling techniques like hard negative mining or uniform sampling.\n",
    "\n",
    "Furthermore, a weird phenomenon we have encountered is that data augmentations have shown to decrease the performance of the model. We have experimented with the simplest augmentation - gaussian noise, and played around with the parameters (assuming the harder augmentations might have occluded the diseases and introduced noise to the data), but the performance of the model was always worse with augmentations. Because the gaussian noise failed, we've decided to not try and use more complicated augmentations like cutmix or mixup and just focus on improving the model architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the currently trained model performs on the test data, the next code block will load the model and make predictions on the test data. The predictions will be saved to a csv file, and it will be displayed here. The block will also display one of the test patient's MRI series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from src.plots import plot_dicom_series\n",
    "\n",
    "\n",
    "def make_submission_with_loaded_multi_model(model: MultiModelSpineCNN, save_suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"makes a submission with the given model and saves it to the submission folder.\"\"\"\n",
    "    submission_csv = pd.DataFrame(columns=[\"row_id\", \"normal_mild\", \"moderate\", \"severe\"])\n",
    "    test_dataset = MultiModelLumbarSpineDataset(False)\n",
    "    dir_path = cfg.SUBMISSION_PATH / f\"{model.name}_submissions\"\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model.load_best_weights()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_dataset)):\n",
    "            test_dict = test_dataset[idx]\n",
    "            test_dict[\"data\"] = [torch.unsqueeze(data, 0) for data in test_dict[\"data\"]]\n",
    "            y_hat = model(test_dict).view(25, 3)  # Only one sample in the test set.\n",
    "            y_hat = F.softmax(y_hat, dim=1)\n",
    "            curr_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"row_id\": [f\"{test_dict['row_id']}_{condition}\" for condition in test_dataset.conditions_i2s],\n",
    "                    \"normal_mild\": y_hat[:, 0].numpy().astype(np.float32),\n",
    "                    \"moderate\": y_hat[:, 1].numpy().astype(np.float32),\n",
    "                    \"severe\": y_hat[:, 2].numpy().astype(np.float32)\n",
    "                }\n",
    "            )\n",
    "            submission_csv = pd.concat([submission_csv, curr_df], axis=0, ignore_index=True)\n",
    "    submission_csv.sort_values(by=\"row_id\", inplace=True)\n",
    "    submission_csv = submission_csv.apply(lambda col: col.astype(np.float32) if col.name != \"row_id\" else col)\n",
    "    save_path = dir_path / f\"{model.name}_{save_suffix}.csv\"\n",
    "    logging.info(f\"Submission saved to {save_path}\")\n",
    "    submission_csv.to_csv(save_path, index=False)\n",
    "    return submission_csv\n",
    "\n",
    "submission_csv = pd.read_csv(cfg.SUBMISSION_PATH / \"submission.csv\")\n",
    "display(submission_csv.head(15))\n",
    "#plot_dicom_series(cfg.TEST_IMAGES_PATH / cfg.TEST_PATIENT_ID / cfg.TEST_PATIENT_SAGITTAL_T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to analyze our trained model. To understand the model's performance, we will produce logits on the entire dataset (our offline evaluation concluded that the results on the training set are the same as on the validation set, practically allmost zero overfit), and then calculate some relevant metrics to mathematically assess the model's prediction ability. The metrics we will use are: \n",
    "\n",
    "- Confusion Matrix - A matrix that shows the number of true positives, true negatives, false positives, and false negatives for each disease. \n",
    "- Precision - The ratio of true positives to the sum of true positives and false positives. This metric indicates how well the model predicts the positive class without predicting too many false positives.\n",
    "- Recall - The ratio of true positives to the sum of true positives and false negatives. This metric indicates how well the model captures the positive class. In our case, this is the most important metric, as we want to capture the diseases as much as possible.\n",
    "- ROC AUC Score - The area under the Receiver Operating Characteristic (ROC) curve. This metric indicates the model's ability to distinguish between the positive and negative classes. A score of 0.5 indicates that the model is not able to distinguish between the classes, while a score of 1 indicates perfect separation.\n",
    "\n",
    "There are additional metrics that are usually displayed when evaluating a model, such as F1 score and Accuracy, but in our case they are not needed since that F1 score is more usefull for comparing different models, and the accuracy is not a good metric for our unbalanced dataset, or generally in medical datasets.\n",
    "\n",
    "The ROC AUC score will only be calculated for the binary task of predicting the presence of the disease (Normal/Mild vs Moderate OR Severe). To make matters simple and be less overwhelmed with information, we'll make an analysis on the binary task first and then the multi-class task. After producing the general metrics, we will examine specific cases of the model's predictions, and try to understand why the model failed in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some libraries we'll be using and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model = MultiModelSpineCNN(sags_args, axial_t2_args, last_fc_dim=1024 + 750, dropout=0.5,\n",
    "                                name=\"do_not_save\")\n",
    "best_model.load_state_dict(torch.load(cfg.BEST_MODEL_PATH))\n",
    "best_model = best_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the model's logits on the training and validation data. We would like to compute logits over the training data as well in order to examine any overfitting that might have occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits_list = []\n",
    "all_targets_list = []\n",
    "all_series_types_list = []\n",
    "is_train_logit_list = [True] * len(train_dataloader) + [False] * len(val_dataloader)\n",
    "\n",
    "num_workers = 6\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "for dataloader in tqdm([train_dataloader, val_dataloader], desc=\"Evaluating sets\"):\n",
    "    with torch.no_grad():\n",
    "        for data_dict in tqdm(dataloader, desc=\"patient\"):\n",
    "            data_dict[\"target\"] = data_dict[\"target\"][0]\n",
    "            data_dict[\"series_types\"] = data_dict[\"series_types\"][0]\n",
    "            data_dict[\"data\"] = [x.to(device) for x in data_dict[\"data\"]]\n",
    "            \n",
    "            logits = best_model(data_dict)  # shape: (1, 25, 3)\n",
    "            \n",
    "            all_logits_list.append(logits.cpu().numpy())\n",
    "            all_targets_list.append(data_dict[\"target\"].cpu().numpy())\n",
    "            \n",
    "            series_types_count = {0: 0, 1: 0}\n",
    "            for series_type in data_dict[\"series_types\"].cpu().numpy():\n",
    "                series_types_count[series_type] += 1\n",
    "            series_types_count = {\"Sagittal\": series_types_count[0], \"Axial\": series_types_count[1]}\n",
    "            all_series_types_list.append(series_types_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all predictions and targets\n",
    "all_logits = np.concatenate(all_logits_list, axis=0)  # shape: (N, 25, 3)\n",
    "all_targets = np.stack(all_targets_list, axis=0)  # shape: (N, 25)\n",
    "\n",
    "# Construct a flattened DataFrame of logits\n",
    "# Each sample will become one row, with columns for every disease-class combination\n",
    "num_samples = all_logits.shape[0]\n",
    "flattened_logits = all_logits.reshape(num_samples, 25 * 3)\n",
    "\n",
    "# Create column names for each (disease, severity) combination\n",
    "columns = []\n",
    "for d_name in MultiModelLumbarSpineDataset.conditions_i2s:\n",
    "    for s_name in MultiModelLumbarSpineDataset.severity_i2s:\n",
    "        columns.append(f\"logits_{d_name}_{s_name}\")\n",
    "\n",
    "series_types_df = pd.DataFrame(data=all_series_types_list)\n",
    "logits_df = pd.DataFrame(data=flattened_logits, columns=columns).merge(series_types_df, left_index=True, right_index=True)\n",
    "logits_df[\"is_train\"] = is_train_logit_list\n",
    "targets_df = pd.DataFrame(data=all_targets, columns=MultiModelLumbarSpineDataset.conditions_i2s)\n",
    "\n",
    "# Combine logits and targets into one DataFrame\n",
    "summary_df = pd.merge(logits_df, targets_df, left_index=True, right_index=True)\n",
    "\n",
    "# Cache the summary DataFrame for future use\n",
    "summary_df.to_pickle(cfg.LOGITS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each disease, we collapse severity into Normal/Mild = 0, \n",
    "Moderate or Severe = 1, then compute metrics across all diseases \n",
    "in a flattened manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_pickle(cfg.LOGITS_OUTPUT_PATH)\n",
    "\n",
    "conditions_i2s = MultiModelLumbarSpineDataset.conditions_i2s\n",
    "severity_i2s   = MultiModelLumbarSpineDataset.severity_i2s\n",
    "\n",
    "NUM_DISEASES = len(conditions_i2s)\n",
    "NUM_CLASSES  = len(severity_i2s)\n",
    "\n",
    "# Identify the logits columns in summary_df\n",
    "logits_columns = [col for col in summary_df.columns if col.startswith(\"logits_\")]\n",
    "\n",
    "# Convert logits columns to a NumPy array and reshape\n",
    "logits_array = summary_df[logits_columns].values  # shape: (N, 25*3)\n",
    "all_logits = logits_array.reshape(-1, NUM_DISEASES, NUM_CLASSES)  # shape: (N, 25, 3)\n",
    "\n",
    "# Extract the target columns (these are named exactly like your conditions)\n",
    "targets_columns = conditions_i2s\n",
    "all_targets = summary_df[targets_columns].values  # shape: (N, 25)\n",
    "\n",
    "print(\"all_logits shape:\", all_logits.shape)\n",
    "print(\"all_targets shape:\", all_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array to torch Tensors for convenience in argmax\n",
    "logits_tensor = torch.from_numpy(all_logits)   # shape: (N, 25, 3)\n",
    "targets_tensor = torch.from_numpy(all_targets) # shape: (N, 25)\n",
    "\n",
    "# Predicted severity index [0..2]\n",
    "preds_tensor = logits_tensor.argmax(dim=-1)  # shape: (N, 25)\n",
    "\n",
    "# Binary threshold: 0 => normal/mild, >=1 => moderate/severe\n",
    "binary_preds = (preds_tensor >= 1).long()\n",
    "binary_targets = (targets_tensor >= 1).long()\n",
    "\n",
    "# Flatten across all diseases\n",
    "bin_preds_flat = binary_preds.reshape(-1).numpy()\n",
    "bin_targets_flat = binary_targets.reshape(-1).numpy()\n",
    "\n",
    "pos_logit = torch.exp(logits_tensor[..., 1].reshape(-1)) + torch.exp(logits_tensor[..., 2].reshape(-1))\n",
    "neg_logit = torch.exp(logits_tensor[..., 0].reshape(-1))\n",
    "\n",
    "prob_positive = pos_logit / (pos_logit + neg_logit)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(bin_targets_flat, bin_preds_flat).ravel()\n",
    "precision_bin  = precision_score(bin_targets_flat, bin_preds_flat)\n",
    "recall_bin     = recall_score(bin_targets_flat, bin_preds_flat)\n",
    "f1_bin         = f1_score(bin_targets_flat, bin_preds_flat)\n",
    "roc_auc_bin    = roc_auc_score(bin_targets_flat, prob_positive)\n",
    "\n",
    "print(\"=== Binary Classification (Normal/Mild vs. Moderate/Severe) ===\")\n",
    "print(\"Confusion Matrix:\\n\", np.array([[tp, fp], [fn, tn]]))\n",
    "print(f\"Precision: {precision_bin:.3f}\")\n",
    "print(f\"Recall:    {recall_bin:.3f}\")\n",
    "print(f\"F1 Score:  {f1_bin:.3f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_bin:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing these simple metrics, we need to fully understand them and analyze them. The confusion matrix is a matrix in the format of\n",
    "$$\n",
    "\\begin{bmatrix} \\#TruePositive & \\#FalsePositive \\\\ \\#FalseNegative & \\#TrueNegative \\end{bmatrix}\n",
    "$$\n",
    "In our case, the confusion matrix resulted in\n",
    "$$\n",
    "\\begin{bmatrix} 37043 & 1283 \\\\ 8444 & 2605 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The confusion matrix shows that out of all samples predicted as negative (Normal/Mild), 37,043 are correctly identified as negatives (True Negatives) while 8,444 are incorrectly labeled as negatives even though they are positives (False Negatives). Among samples predicted as positive (Moderate/Severe), 2,605 are correctly identified (True Positives) whereas 1,283 are actually negatives labeled as positives (False Positives). The precision of 0.670 indicates that when the model classifies a sample as Moderate/Severe, about 67% of the time it is correct. The recall of 0.236 means that out of all the actual Moderate/Severe samples, the model only captures about 23.6% of them, suggesting a tendency to miss many positive cases (leading to high False Negatives). The F1 score of 0.349, which combines precision and recall, is relatively low, reflecting an imbalance between how many positive samples are correctly identified and how many are missed. Nevertheless, the ROC AUC of 0.826 suggests that, when looking at the full spectrum of possible decision thresholds (rather than the single threshold used here), the model is reasonably good at ranking or distinguishing between positives and negatives. Possible causes for these results include class imbalance (if Normal/Mild cases are more frequent, the model may learn to lean towards predicting negative), suboptimal threshold settings (the default cutoff at 0.5 for a binary classification might not be ideal), or insufficient representativeness of the training data for Moderate/Severe examples. Improving performance could involve collecting and labeling more balanced training samples, or oversampling for the underrepresented classes, adjusting decision thresholds to optimize recall, and employing more advanced architectures or hyperparameter tuning to focus on correctly identifying Moderate/Severe cases.\n",
    "\n",
    "Additionally, it may be that the model’s feature representation for more severe pathologies is insufficient, something the might have occured due to the down-scaled input images. We downscaled each 2D slice to $(128, 171)$ because of our limited computational resources and the competitions' training time limit. This size is a measly about one fourth of the minimum possible slice size, so we probably are losing much of the fine-grained detail, which is makes it difficult for the model the find the conditions, leading to the high False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue to the Multi-Class Evaluation. We will compute metrics for the full three-class problem \n",
    "(Normal/Mild=0, Moderate=1, Severe=2). Rather than collapsing to \n",
    "a binary task, we'll look at the three classes across **all** diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 'preds_tensor' has shape (N, 25), each in [0..2]\n",
    "# 'targets_tensor' has shape (N, 25), each in [0..2]\n",
    "\n",
    "# Flatten across all diseases for an overall confusion matrix\n",
    "multi_preds_flat = preds_tensor.reshape(-1).numpy()    # shape: (N*25,)\n",
    "multi_targets_flat = targets_tensor.reshape(-1).numpy()  # shape: (N*25,)\n",
    "\n",
    "# Generate confusion matrix for the 3 classes\n",
    "cm_multi = confusion_matrix(multi_targets_flat, multi_preds_flat, labels=[0,1,2])\n",
    "\n",
    "# Print classification report with macro, weighted, or micro averaging\n",
    "# (this provides per-class precision, recall, F1)\n",
    "print(\"=== Multi-Class Classification Report (All Diseases) ===\")\n",
    "print(classification_report(\n",
    "    multi_targets_flat,\n",
    "    multi_preds_flat,\n",
    "    labels=[0,1,2],\n",
    "    target_names=severity_i2s,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Plot the confusion matrix using seaborn for a more appealing style\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cm_multi, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=severity_i2s, \n",
    "            yticklabels=severity_i2s, \n",
    "            cbar=False, \n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title(\"Overall 3-Class Confusion Matrix (Flattened Across All Diseases)\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix clearly shows that the model frequently misclassifies both Moderate and Severe cases as Normal/Mild, and there is also noticeable confusion between Moderate and Severe themselves. This overlap between the two more serious categories is not surprising given that distinguishing borderline Moderate from a more Severe state often depends on subtle clinical indicators, making it harder for the model to draw a clean boundary. Moreover, although the model achieves higher precision than recall, it means that when the model does predict a given condition (Moderate or Severe), it is correct at a relatively higher rate, but it misses a large portion of actual Moderate/Severe cases. In the context of diagnosing conditions, precision speaks to how often a predicted Moderate/Severe diagnosis is truly Moderate/Severe, whereas recall reflects how many of all existing Moderate/Severe cases the model manages to catch. Consequently, while the model is cautious about labeling something as Moderate or Severe (leading to fewer false positives and thus higher precision), it fails to capture many patients who genuinely belong to those more serious categories, which in a real clinical scenario could mean a higher risk of missing patients requiring urgent attention. This strenghtens our suggestion to find a more suitable threshold for the model, since there is more damage in missing a patient than diagnosing a healthy one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute multi-class confusion matrices and metrics for each individual disease. We'll store them in a DataFrame for easy inspection. Then we can visualize a few selected diseases in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "disease_metrics_list = []\n",
    "for d_idx, disease_name in enumerate(conditions_i2s):\n",
    "    # Extract predictions and targets for a single disease\n",
    "    disease_preds = preds_tensor[:, d_idx].numpy()   # shape: (N,)\n",
    "    disease_true  = targets_tensor[:, d_idx].numpy() # shape: (N,)\n",
    "    \n",
    "    # Compute confusion matrix & metrics for this disease\n",
    "    cm = confusion_matrix(disease_true, disease_preds, labels=[0,1,2])\n",
    "    prec = precision_score(disease_true, disease_preds, average='weighted', zero_division=0)\n",
    "    rec  = recall_score(disease_true, disease_preds, average='weighted', zero_division=0)\n",
    "    f1   = f1_score(disease_true, disease_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    disease_metrics_list.append({\n",
    "        \"disease\": disease_name,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "multi_metrics_df = pd.DataFrame(disease_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 22\n",
    "example_disease = multi_metrics_df.loc[example_idx, \"disease\"]\n",
    "example_cm = multi_metrics_df.loc[example_idx, \"confusion_matrix\"]\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(\n",
    "    example_cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "    xticklabels=severity_i2s, \n",
    "    yticklabels=severity_i2s,\n",
    "    cbar=False\n",
    ")\n",
    "plt.title(f\"Confusion Matrix for {example_disease}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result falls in line with the overall multi-class evaluation, showing that the model struggles to distinguish between Moderate and Sever, while tending to falsely predict Normal/Mild for both. The confusion matrix and metrics for each disease provide a more granular view of the model's performance, highlighting specific areas where it excels or falls short. For instance, the model performs relatively well on diseases like right_subarticular_stenosis_l3_l4, where it achieves a bit higher recall and precision compared to other diseases. On the other hand, diseases like left_neural_foraminal_narrowing_l4_l5 show a higher number of false negatives, indicating that the model often misses these cases. These insights can guide further model refinement, such as adjusting decision thresholds, exploring more advanced architectures, or collecting more diverse training data to improve performance on specific diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to plot 3 images of a patient.  \n",
    "Our goal is to compare the model predictions to the real data and take a look at samples which the model has difficulty with.  \n",
    "In our sample we'll pick the diseases \"spinal_canal_stenosis_l1_l2\", \"left_neural_foraminal_narrowing_l1_l2\" and \"left_subarticular_stenosis_l1_l2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see an instance where the model is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p218 = val_dataset[218][\"data\"][0]\n",
    "img1 = val_p218[8][0]\n",
    "img2 = val_p218[12][0]\n",
    "img3 = val_p218[15][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [img1, img2, img3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "title = \"patient 218 from validation set\"\n",
    "\n",
    "for ax, img in zip(axes, images):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle(title, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets load the model data from 'summary_df' and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types = [\n",
    "    \"spinal_canal_stenosis_l1_l2\",\n",
    "    \"left_neural_foraminal_narrowing_l1_l2\",\n",
    "    \"right_subarticular_stenosis_l3_l4\"\n",
    "]\n",
    "\n",
    "def print_result(patient: int):\n",
    "    logits_columns = [\n",
    "        f\"logits_{disease_types[0]}_Normal/Mild\", f\"logits_{disease_types[0]}_Moderate\", f\"logits_{disease_types[0]}_Severe\",\n",
    "        f\"logits_{disease_types[1]}_Normal/Mild\", f\"logits_{disease_types[1]}_Moderate\", f\"logits_{disease_types[1]}_Severe\",\n",
    "        f\"logits_{disease_types[2]}_Normal/Mild\", f\"logits_{disease_types[2]}_Moderate\", f\"logits_{disease_types[2]}_Severe\"\n",
    "    ]\n",
    "\n",
    "    val_df = summary_df[~summary_df[\"is_train\"]].reset_index(drop=True)\n",
    "    logits = np.array(val_df.loc[patient, logits_columns], dtype=np.float32).reshape(len(disease_types), 3)\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
    "\n",
    "    # Get the predicted classes\n",
    "    class_labels = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n",
    "    predicted_classes = [class_labels[np.argmax(prob)] for prob in probabilities]\n",
    "    target_classes = [severity_i2s[i] for i in val_df.loc[patient, disease_types]]\n",
    "\n",
    "    print(f\"Patient {patient} Probabilities & Ground Truth:\")\n",
    "    for i, disease in enumerate(disease_types):\n",
    "        print(f\"{disease}: Target Class = {target_classes[i]}, Predicted Class = {predicted_classes[i]}, \"\n",
    "            f\"\\nProbabilities = {probabilities[i]}\", end=\"\\n\\n\")\n",
    "        \n",
    "print_result(218)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is indeed correct with high confidence. It can also be seen in the MRI that a condition is present in the lower part of the spine. This is a good result, the model predicted Normal/Mild where no condition was present, and Severe when an actual severe condition was present. Let's take a look at another sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p291 = val_dataset[291][\"data\"][1]\n",
    "img1 = val_p291[7][0]\n",
    "img2 = val_p291[12][0]\n",
    "img3 = val_p291[16][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [img1, img2, img3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "title = \"patient 291 from validation set\"\n",
    "\n",
    "for ax, img in zip(axes, images):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle(title, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(291)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model predicted Normal/Mild, for two of the diseases, and Moderate for the third. The actual conditions are Normal/Mild for all three of them, meaning we got a false positive for the third disease. This is quite odd to see, as we expect more false negatives than false positives. After inspecting the MRI that was plotted above, it does look like the patient is healthy, and the model was correct in predicting Normal/Mild for the first two diseases. The false positive we got might be due to the fact that the MRI is particularly dark, where the conditions usually are also seen as dark spots. Maybe the low birghtness of the MRI confused the model. This indicates that brightness augmentation might have been a good idea to try out, if this hypothesis is really correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have explored the task of predicting the severity of specific lumbar spinal pathologies from MRI data and documented our entire workflow, from initial exploratory data analysis (EDA) to final model evaluation. We began by laying out the rationale and clinical motivation: accurately determining whether a condition is Normal/Mild, Moderate, or Severe can guide treatment plans, reduce unnecessary procedures, and improve patient outcomes. Our preliminary data inspections revealed that certain diagnoses were more common than others, leading us to assess class distribution, potential anomalies, and patterns within the dataset. This EDA phase was crucial not only for identifying imbalances that could skew model training but also for helping us pick out the relevant features and confirm that each MRI series had sufficient variations in image quality, slice thickness, and fields of view.\n",
    "\n",
    "Subsequently, we walked through our algorithmic pipeline, beginning with data handling and preprocessing. We showed how we carefully restructured the dataset to ensure that each MRI was consistently loaded and preprocessed. This approach, particularly in medical imaging, can help the model generalize by exposing it to a wide range of possible variations (such as brightness and contrast differences). Moreover, we described our model architecture, which combined multiple imaging perspectives - Sagittal and Axial views to give the neural network a more holistic understanding of each spinal segment. At the core of this architecture was a feature-extraction backbone (resnet 3D 18) tailored to capture fine-grained details indicative of subtle disease progression.\n",
    "\n",
    "Once the training procedure was established, including hyperparameter choices and cross-validation seeds, we delved into inference and metric reporting. We designed a unified pipeline that performed both multi-class classification (Normal/Mild vs. Moderate vs. Severe) and a binary classification variant (Normal/Mild vs. Moderate/Severe). We then generated comprehensive metrics, including confusion matrices, precision, recall, F1 scores, and ROC AUC - both on an aggregate level across all diseases and on a disease-by-disease basis. Through these results, we could pinpoint conditions that were particularly challenging to classify, as well as examine the consistent tendency of the model to predict Normal/Mild for ambiguous cases. Our extended discussion of these metrics underscores the importance of interpretability in the medical context: it is not enough simply to have a high accuracy, but we must also consider whether the model is missing critical Moderate/Severe cases or over - diagnosing normal patients.\n",
    "\n",
    "In our deeper investigation, we addressed issues that often arise in real-world medical imaging applications, such as class imbalance and overlapping clinical presentations. Although we deliberately avoided repeating the details of our balancing strategies here in the summary, it was evident from the EDA and training logs that some form weighting was helpful to prevent the network from collapsing to predict only the majority class. Further, our confusion matrices highlighted the difficulty in distinguishing border cases between Moderate and Severe categories, a situation common in clinical practice where a borderline disc protrusion or mild canal stenosis may be hard to differentiate from a more pronounced pathology.\n",
    "\n",
    "Finally, we presented an approach to storing logits and targets in a dedicated DataFrame, enabling rapid post-hoc analysis without the overhead of re-running inference across the validation or test sets. This design decision proved invaluable for iterative experimentation, letting us adjust metrics, thresholds, and visualizations on the fly. By demonstrating clear, aesthetically appealing plots of our results - especially multi-class confusion matrices and per-disease breakdowns, we showed how these diagnostic tools can inform further refinements in model design and clinical decision thresholds. Such analyses not only offer immediate insights to clinicians but also guide the next steps in research, whether that means incorporating additional MRI sequences, refining data pre-processing strategies, or performing more advanced hyperparameter optimization.\n",
    "\n",
    "Altogether, this project underscores both the promise and the challenges of applying deep learning to spinal MRI interpretation. The combination of rich medical context and flexible computational tools has the potential to reduce diagnostic variability and speed up clinical workflows. Our multi-modal approach, covering EDA, data handling, network architecture, evaluation metrics, and beyond - presents a replicable template for further research. In practical settings, we envision that the pipeline could be refined and validated with more extensive multi-institutional data, followed by prospective studies to confirm whether model outputs genuinely translate into better patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workload Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of us (Ofir and Netanel) have contributed equally to this notebook. We have worked together on the EDA, algorithmic approach, experimentation, and model analysis. We have also shared the workload of writing the notebook, and we have both contributed to the final summary. We have both learned a lot from this project, and we are proud of the results we have achieved. We are excited to continue working together on future projects and to continue learning and growing as data scientists. Though we both contributed to every aspect of this project, one teammate had a general responsibility for each section. Netanel was responsible for the EDA and researching about the diseases, while Ofir was responsible for the design of the notebook (TOD, 3D plots, etc.) and the algorithmic approach with the model analysis. We have both contributed to the experimentation and the final summary, as it required the insights of both of us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to present a few extra leads we found during our online research.\n",
    "1. [DeepSPINE](https://arxiv.org/abs/1807.10215) and [Tumko et al.](https://link.springer.com/article/10.1007/s00586-023-08089-2) - Some of the motivations for the idea of ROI detection and classification.\n",
    "2. [Monai Models](https://monai.io/model-zoo.html) - A collection of pre-trained models for medical image analysis. After seeing that the PyTorch's provided pre-trained models did not help much, we tried researching domain specific pre-trained models. We have found this collection, but ultimatly did not use it.\n",
    "3. [SpineNet](https://github.com/rwindsor1/SpineNet) - An ROI model like the one used in DeepSPINE. We have found this model after deciding to not use the ROI detection and classification approach, but it might be useful for future research.\n",
    "4. [Hugging Face](https://huggingface.co/), [guidance blog](https://learnopencv.com/medical-image-segmentation/) - More pre-trained models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
